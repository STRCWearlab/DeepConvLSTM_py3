{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Opportunity challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and preprocess data\n",
    "\n",
    "First, download the dataset for the opportunity challenge and set up the directories to hold the raw and processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-28 12:35:53--  https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 306636009 (292M) [application/x-httpd-php]\n",
      "Saving to: ‘OpportunityUCIDataset.zip’\n",
      "\n",
      "OpportunityUCIDatas 100%[===================>] 292.43M  9.14MB/s    in 51s     \n",
      "\n",
      "2021-09-28 12:36:44 (5.77 MB/s) - ‘OpportunityUCIDataset.zip’ saved [306636009/306636009]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
    "! mkdir -p data/raw\n",
    "! mv OpportunityUCIDataset.zip data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following script to preprocess the data. We collate all the training data into one file, same for the validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset data/raw/OpportunityUCIDataset.zip\n",
      "Processing dataset files ...\n",
      "Generating training files\n",
      "... file OpportunityUCIDataset/dataset/S1-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL3.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL4.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL3.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL3.dat -> train_data\n",
      "Generating validation files\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL5.dat -> val_data\n",
      "Generating testing files\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL4.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL5.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL4.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL5.dat -> test_data\n",
      "[*] Reading raw files from data/opportunity\n",
      "[-] Train data : (43985, 24, 113) float32, target (43985,) int64\n",
      "[-] Valid data : (2509, 24, 113) float32, target (2509,) int64\n",
      "[-] Test data : (9894, 24, 113) float32, target (9894,) int64\n",
      "[-] Test data sample-wise : (118726, 24, 113) float32, target sample-wise (118726,) int64\n",
      "[+] Processed segment datasets successfully saved!\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 preprocess_opportunity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Config\n",
    "\n",
    "We choose the parameters of our sliding window, and specify the name and location of the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp_class_names = ['Null', 'Open Door 1', 'Open Door 2', 'Close Door 1', 'Close Door 2', 'Open Fridge',\n",
    "                       'Close Fridge', 'Open Dishwasher', 'Close Dishwasher', 'Open Drawer 1', 'Close Drawer 1',\n",
    "                       'Open Drawer 2', 'Close Drawer 2',\n",
    "                       'Open Drawer 3', 'Close Drawer 3', 'Clean Table', 'Drink from Cup', 'Toggle Switch']\n",
    "\n",
    "target_dataset = 'opportunity'\n",
    "window_size = 24\n",
    "window_step = 12\n",
    "n_classes = 18\n",
    "\n",
    "config_dataset = {\n",
    "        \"dataset\": target_dataset,\n",
    "        \"window\": window_size,\n",
    "        \"stride\": window_step,\n",
    "        \"stride_test\": 1,\n",
    "        \"path_processed\": f\"data/{target_dataset}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data for training and validation (see datasets.py for sensor dataset implementation). We get the number of channels from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mCreating opportunity train HAR dataset of size 43985 ...\u001b[0m\n",
      "\u001b[92mCreating opportunity val HAR dataset of size 2509 ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import SensorDataset\n",
    "dataset = SensorDataset(**config_dataset, prefix=\"train\")\n",
    "dataset_val = SensorDataset(**config_dataset, prefix=\"val\")\n",
    "n_channels = dataset.n_channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import DeepConvLSTM class\n",
    "\n",
    "See DeepConvLSTM_py3.py for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepConvLSTM_py3 import DeepConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an instance of DeepConvLSTM, with the number of channels and classes defined earlier. \n",
    "The dataset arg determines where the results and training checkpoints should be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepconv = DeepConvLSTM(n_channels=n_channels, n_classes=n_classes, dataset=target_dataset).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train for only 30 epochs here as a demonstration. We use a learning rate scheduler to decrease the maximum learning rate for all parameters every 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mRunning HAR training loop ...\u001b[0m\n",
      "\u001b[92m[-] Initializing weights (orthogonal)...\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 0/30\tTrain loss: 1.03 \tacc: 72.24(%)\tfm: 9.18(%)\tfw: 64.31(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 0/30\tVal loss: 0.66 \tacc: 83.62(%)\tfm: 5.12(%)\tfw: 77.42(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.0->0.05116573992781191)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 1/30\tTrain loss: 0.74 \tacc: 76.68(%)\tfm: 22.13(%)\tfw: 72.53(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 1/30\tVal loss: 0.52 \tacc: 83.98(%)\tfm: 18.83(%)\tfw: 82.15(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.05116573992781191->0.18828245296798587)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 2/30\tTrain loss: 0.63 \tacc: 78.49(%)\tfm: 27.99(%)\tfw: 74.37(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 2/30\tVal loss: 0.55 \tacc: 83.94(%)\tfm: 21.72(%)\tfw: 82.62(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.18828245296798587->0.2171869488536155)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 3/30\tTrain loss: 0.55 \tacc: 80.63(%)\tfm: 36.78(%)\tfw: 78.15(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 3/30\tVal loss: 0.41 \tacc: 84.97(%)\tfm: 30.24(%)\tfw: 84.46(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.2171869488536155->0.3023972366088422)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 4/30\tTrain loss: 0.48 \tacc: 82.97(%)\tfm: 46.79(%)\tfw: 81.21(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 4/30\tVal loss: 0.35 \tacc: 88.32(%)\tfm: 37.20(%)\tfw: 87.35(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.3023972366088422->0.37204191199665)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 5/30\tTrain loss: 0.48 \tacc: 82.77(%)\tfm: 44.74(%)\tfw: 80.90(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 5/30\tVal loss: 0.38 \tacc: 88.04(%)\tfm: 36.85(%)\tfw: 87.18(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 6/30\tTrain loss: 0.44 \tacc: 83.66(%)\tfm: 47.75(%)\tfw: 82.16(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 6/30\tVal loss: 0.44 \tacc: 86.29(%)\tfm: 37.92(%)\tfw: 85.71(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.37204191199665->0.3792257459584951)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 7/30\tTrain loss: 0.43 \tacc: 84.00(%)\tfm: 48.06(%)\tfw: 82.01(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 7/30\tVal loss: 0.40 \tacc: 86.25(%)\tfm: 33.78(%)\tfw: 85.23(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 8/30\tTrain loss: 0.38 \tacc: 86.08(%)\tfm: 58.39(%)\tfw: 85.54(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 8/30\tVal loss: 0.40 \tacc: 87.17(%)\tfm: 40.95(%)\tfw: 86.90(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.3792257459584951->0.4094855386827936)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 9/30\tTrain loss: 0.36 \tacc: 86.53(%)\tfm: 56.71(%)\tfw: 85.39(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 9/30\tVal loss: 0.42 \tacc: 86.45(%)\tfm: 40.65(%)\tfw: 86.05(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 10/30\tTrain loss: 0.34 \tacc: 86.73(%)\tfm: 57.35(%)\tfw: 85.73(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 10/30\tVal loss: 0.36 \tacc: 87.60(%)\tfm: 44.31(%)\tfw: 87.35(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.4094855386827936->0.4430974523120741)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 11/30\tTrain loss: 0.33 \tacc: 87.63(%)\tfm: 60.29(%)\tfw: 86.84(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 11/30\tVal loss: 0.34 \tacc: 87.64(%)\tfm: 43.72(%)\tfw: 87.24(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 12/30\tTrain loss: 0.31 \tacc: 88.56(%)\tfm: 66.62(%)\tfw: 88.14(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 12/30\tVal loss: 0.31 \tacc: 89.32(%)\tfm: 46.13(%)\tfw: 88.78(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.4430974523120741->0.46127313371128686)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 13/30\tTrain loss: 0.30 \tacc: 89.00(%)\tfm: 67.01(%)\tfw: 88.89(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 13/30\tVal loss: 0.40 \tacc: 87.05(%)\tfm: 48.41(%)\tfw: 87.61(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.46127313371128686->0.48414394905820707)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 14/30\tTrain loss: 0.28 \tacc: 89.78(%)\tfm: 70.52(%)\tfw: 89.77(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 14/30\tVal loss: 0.32 \tacc: 89.32(%)\tfm: 56.12(%)\tfw: 89.45(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.48414394905820707->0.5611505909284105)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 15/30\tTrain loss: 0.28 \tacc: 89.78(%)\tfm: 70.13(%)\tfw: 89.63(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 15/30\tVal loss: 0.34 \tacc: 88.76(%)\tfm: 49.86(%)\tfw: 88.54(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 16/30\tTrain loss: 0.25 \tacc: 90.97(%)\tfm: 73.60(%)\tfw: 90.81(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 16/30\tVal loss: 0.35 \tacc: 89.08(%)\tfm: 52.04(%)\tfw: 88.93(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 17/30\tTrain loss: 0.26 \tacc: 90.39(%)\tfm: 70.47(%)\tfw: 89.91(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 17/30\tVal loss: 0.31 \tacc: 89.96(%)\tfm: 50.74(%)\tfw: 89.27(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 18/30\tTrain loss: 0.24 \tacc: 91.05(%)\tfm: 74.01(%)\tfw: 90.90(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 18/30\tVal loss: 0.31 \tacc: 88.92(%)\tfm: 54.30(%)\tfw: 89.05(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 19/30\tTrain loss: 0.24 \tacc: 91.03(%)\tfm: 72.15(%)\tfw: 90.42(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 19/30\tVal loss: 0.31 \tacc: 90.51(%)\tfm: 51.79(%)\tfw: 89.43(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 20/30\tTrain loss: 0.22 \tacc: 92.39(%)\tfm: 78.27(%)\tfw: 92.35(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 20/30\tVal loss: 0.27 \tacc: 91.67(%)\tfm: 59.79(%)\tfw: 91.54(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5611505909284105->0.597884248923796)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 21/30\tTrain loss: 0.22 \tacc: 92.32(%)\tfm: 78.48(%)\tfw: 92.38(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 21/30\tVal loss: 0.35 \tacc: 88.72(%)\tfm: 57.17(%)\tfw: 89.42(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 22/30\tTrain loss: 0.23 \tacc: 91.81(%)\tfm: 79.24(%)\tfw: 91.74(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 22/30\tVal loss: 0.40 \tacc: 88.04(%)\tfm: 57.99(%)\tfw: 88.45(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 23/30\tTrain loss: 0.21 \tacc: 92.75(%)\tfm: 80.46(%)\tfw: 92.82(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 23/30\tVal loss: 0.32 \tacc: 90.35(%)\tfm: 59.19(%)\tfw: 90.65(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 24/30\tTrain loss: 0.19 \tacc: 93.36(%)\tfm: 82.17(%)\tfw: 93.25(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 24/30\tVal loss: 0.30 \tacc: 90.35(%)\tfm: 57.43(%)\tfw: 90.12(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 25/30\tTrain loss: 0.18 \tacc: 93.49(%)\tfm: 83.31(%)\tfw: 93.45(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 25/30\tVal loss: 0.42 \tacc: 87.96(%)\tfm: 58.86(%)\tfw: 88.52(%)\tinf:0\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 26/30\tTrain loss: 0.19 \tacc: 93.53(%)\tfm: 83.22(%)\tfw: 93.50(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 26/30\tVal loss: 0.35 \tacc: 89.00(%)\tfm: 60.20(%)\tfw: 89.34(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.597884248923796->0.6020313888482283)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 27/30\tTrain loss: 0.17 \tacc: 94.37(%)\tfm: 85.88(%)\tfw: 94.40(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 27/30\tVal loss: 0.33 \tacc: 89.76(%)\tfm: 62.76(%)\tfw: 90.03(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6020313888482283->0.6276028832073217)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 28/30\tTrain loss: 0.15 \tacc: 94.76(%)\tfm: 86.29(%)\tfw: 94.78(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 28/30\tVal loss: 0.34 \tacc: 89.92(%)\tfm: 65.55(%)\tfw: 90.43(%)\tinf:0\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6276028832073217->0.6555488724073396)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "\u001b[92m[-] Epoch 29/30\tTrain loss: 0.16 \tacc: 94.49(%)\tfm: 86.78(%)\tfw: 94.54(%)\tinf:0\u001b[0m\n",
      "\u001b[92m[-] Epoch 29/30\tVal loss: 0.34 \tacc: 89.88(%)\tfm: 63.90(%)\tfw: 90.26(%)\tinf:0\u001b[0m\n",
      "\u001b[92mFinished HAR training loop (h:m:s): 0:07:18\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from DeepConvLSTM_py3 import model_train\n",
    "\n",
    "# Define train config options\n",
    "config_train = {'batch_size': 256,\n",
    "                'optimizer': 'Adam',\n",
    "                'lr': 1e-3,\n",
    "                'lr_step': 100,\n",
    "                'lr_decay': 0.1,\n",
    "                'init_weights': 'orthogonal',\n",
    "                'epochs': 30,\n",
    "                'clip_grad': 0,\n",
    "                'print_freq': 100,\n",
    "                'num_batches': 212,\n",
    "                'num_batches_eval': 212}\n",
    "\n",
    "model_train(deepconv, dataset, dataset_val, config_train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data to evaluate the trained model, and setup the test configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mCreating opportunity test HAR dataset of size 9894 ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset_test = SensorDataset(**config_dataset, prefix=\"test\")\n",
    "test_config = {'batch_size': 256,\n",
    "              'train_mode': False,\n",
    "              'dataset': target_dataset,\n",
    "              'num_batches_eval': 212}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model_eval and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mRunning HAR evaluation loop ...\u001b[0m\n",
      "[-] Loading checkpoint ...\n",
      "\u001b[92m[-] Test loss: 0.40\tacc: 87.71(%)\tfm: 54.94(%)\tfw: 87.87(%)\u001b[0m\n",
      "\u001b[92m[Finished HAR evaluation loop (h:m:s): 0:00:01\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from DeepConvLSTM_py3 import model_eval\n",
    "model_eval(deepconv, dataset_test, test_config, return_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use [thop](https://pypi.org/project/thop/) to count the number of floating point operations performed during a forward pass of one batch of synthetic data. We divide by the batch size to get the number of operations per window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_lstm() for <class 'torch.nn.modules.rnn.LSTM'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'DeepConvLSTM_py3.DeepConvLSTM'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "Number of floating point operations in a forward pass of one batch: 115671040.0\n"
     ]
    }
   ],
   "source": [
    "# Get number of flops\n",
    "from thop import profile\n",
    "import torch\n",
    "deepconv = deepconv.train()\n",
    "\n",
    "x = torch.ones([config_train['batch_size'], config_dataset['window'], n_channels]).cuda()\n",
    "macs, params = profile(deepconv, inputs=(x,), verbose=True)\n",
    "flops = macs / config_train['batch_size']\n",
    "print(f'Number of floating point operations in a forward pass of one batch: {flops}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
