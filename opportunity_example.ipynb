{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Opportunity challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and preprocess data\n",
    "\n",
    "First, download the dataset for the opportunity challenge and set up the directories to hold the raw and processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-29 12:50:26--  https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 306636009 (292M) [application/x-httpd-php]\n",
      "Saving to: ‘OpportunityUCIDataset.zip’\n",
      "\n",
      "OpportunityUCIDatas 100%[===================>] 292.43M  8.53MB/s    in 66s     \n",
      "\n",
      "2021-09-29 12:51:33 (4.44 MB/s) - ‘OpportunityUCIDataset.zip’ saved [306636009/306636009]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
    "! mkdir -p data/raw\n",
    "! mv OpportunityUCIDataset.zip data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following script to preprocess the data. We collate all the training data into one file, same for the validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset data/raw/OpportunityUCIDataset.zip\n",
      "Processing dataset files ...\n",
      "Generating training files\n",
      "... file OpportunityUCIDataset/dataset/S1-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL3.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL4.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL3.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL3.dat -> train_data\n",
      "Generating validation files\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL5.dat -> val_data\n",
      "Generating testing files\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL4.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL5.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL4.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL5.dat -> test_data\n",
      "[*] Reading raw files from data/opportunity\n",
      "[-] Train data : (43985, 24, 113) float32, target (43985,) int64\n",
      "[-] Valid data : (2509, 24, 113) float32, target (2509,) int64\n",
      "[-] Test data : (9894, 24, 113) float32, target (9894,) int64\n",
      "[-] Test data sample-wise : (118726, 24, 113) float32, target sample-wise (118726,) int64\n",
      "[+] Processed segment datasets successfully saved!\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 preprocess_opportunity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Config\n",
    "\n",
    "We choose the parameters of our sliding window, and specify the name and location of the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = 'opportunity'\n",
    "window_size = 24\n",
    "window_step = 12\n",
    "n_classes = 18\n",
    "\n",
    "config_dataset = {\n",
    "        \"dataset\": target_dataset,\n",
    "        \"window\": window_size,\n",
    "        \"stride\": window_step,\n",
    "        \"stride_test\": 1,\n",
    "        \"path_processed\": f\"data/{target_dataset}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data for training and validation (see datasets.py for sensor dataset implementation). We get the number of channels from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mCreating opportunity train HAR dataset of size 43985 ...\u001b[0m\n",
      "\u001b[92mCreating opportunity val HAR dataset of size 2509 ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import SensorDataset\n",
    "dataset = SensorDataset(**config_dataset, prefix=\"train\")\n",
    "dataset_val = SensorDataset(**config_dataset, prefix=\"val\")\n",
    "n_channels = dataset.n_channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import DeepConvLSTM class\n",
    "\n",
    "See DeepConvLSTM_py3.py for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepConvLSTM_py3 import DeepConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an instance of DeepConvLSTM, with the number of channels and classes defined earlier. \n",
    "The dataset arg determines where the results and training checkpoints should be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepconv = DeepConvLSTM(n_channels=n_channels, n_classes=n_classes, dataset=target_dataset).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train for only 30 epochs here as a demonstration. We use a learning rate scheduler to decrease the maximum learning rate for all parameters every 10 epochs, by a factor of 0.9. Check model_train docstring for explanation of the config keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mRunning HAR training loop ...\u001b[0m\n",
      "\u001b[92m[-] Initializing weights (orthogonal)...\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 2.895446\n",
      "[-] Batch 100/172\t Loss: 1.408444\n",
      "\u001b[92m[-] Epoch 0/30\tTrain loss: 1.00 \tacc: 73.26(%)\tfm: 10.31(%)\tfw: 64.02(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 0/30\tVal loss: 0.62 \tacc: 83.38(%)\tfm: 9.96(%)\tfw: 78.61(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.0->0.09962714055342088)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 1.199953\n",
      "[-] Batch 100/172\t Loss: 0.913685\n",
      "\u001b[92m[-] Epoch 1/30\tTrain loss: 0.75 \tacc: 77.66(%)\tfm: 22.82(%)\tfw: 72.01(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 1/30\tVal loss: 0.47 \tacc: 87.41(%)\tfm: 21.40(%)\tfw: 83.77(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.09962714055342088->0.2139574317938737)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.718886\n",
      "[-] Batch 100/172\t Loss: 0.739933\n",
      "\u001b[92m[-] Epoch 2/30\tTrain loss: 0.68 \tacc: 76.83(%)\tfm: 22.46(%)\tfw: 72.58(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 2/30\tVal loss: 0.46 \tacc: 83.10(%)\tfm: 18.90(%)\tfw: 81.58(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.699283\n",
      "[-] Batch 100/172\t Loss: 0.646833\n",
      "\u001b[92m[-] Epoch 3/30\tTrain loss: 0.58 \tacc: 79.60(%)\tfm: 30.12(%)\tfw: 75.58(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 3/30\tVal loss: 0.37 \tacc: 87.45(%)\tfm: 23.11(%)\tfw: 85.37(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.2139574317938737->0.23105829246263068)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.588908\n",
      "[-] Batch 100/172\t Loss: 0.574378\n",
      "\u001b[92m[-] Epoch 4/30\tTrain loss: 0.50 \tacc: 82.84(%)\tfm: 42.06(%)\tfw: 80.14(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 4/30\tVal loss: 0.36 \tacc: 89.00(%)\tfm: 32.59(%)\tfw: 87.02(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.23105829246263068->0.32590871161787516)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.405352\n",
      "[-] Batch 100/172\t Loss: 0.500100\n",
      "\u001b[92m[-] Epoch 5/30\tTrain loss: 0.43 \tacc: 84.48(%)\tfm: 54.22(%)\tfw: 83.29(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 5/30\tVal loss: 0.35 \tacc: 87.84(%)\tfm: 43.86(%)\tfw: 87.13(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.32590871161787516->0.4385951100419649)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.493169\n",
      "[-] Batch 100/172\t Loss: 0.424954\n",
      "\u001b[92m[-] Epoch 6/30\tTrain loss: 0.37 \tacc: 86.55(%)\tfm: 58.11(%)\tfw: 85.56(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 6/30\tVal loss: 0.38 \tacc: 87.05(%)\tfm: 48.65(%)\tfw: 87.20(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.4385951100419649->0.48652800118864736)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.437551\n",
      "[-] Batch 100/172\t Loss: 0.356062\n",
      "\u001b[92m[-] Epoch 7/30\tTrain loss: 0.32 \tacc: 88.58(%)\tfm: 66.72(%)\tfw: 88.28(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 7/30\tVal loss: 0.31 \tacc: 89.36(%)\tfm: 52.02(%)\tfw: 89.17(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.48652800118864736->0.5202175819028652)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.321276\n",
      "[-] Batch 100/172\t Loss: 0.301637\n",
      "\u001b[92m[-] Epoch 8/30\tTrain loss: 0.27 \tacc: 90.37(%)\tfm: 70.22(%)\tfw: 90.14(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 8/30\tVal loss: 0.34 \tacc: 88.48(%)\tfm: 53.33(%)\tfw: 88.95(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5202175819028652->0.5332958232121431)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.256679\n",
      "[-] Batch 100/172\t Loss: 0.276700\n",
      "\u001b[92m[-] Epoch 9/30\tTrain loss: 0.22 \tacc: 92.45(%)\tfm: 78.37(%)\tfw: 92.28(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 9/30\tVal loss: 0.35 \tacc: 89.12(%)\tfm: 55.74(%)\tfw: 89.20(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5332958232121431->0.5573938034595434)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.284003\n",
      "[-] Batch 100/172\t Loss: 0.231458\n",
      "\u001b[92m[-] Epoch 10/30\tTrain loss: 0.19 \tacc: 93.35(%)\tfm: 81.90(%)\tfw: 93.27(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 10/30\tVal loss: 0.26 \tacc: 90.43(%)\tfm: 56.20(%)\tfw: 90.47(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5573938034595434->0.5619860053133207)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.211770\n",
      "[-] Batch 100/172\t Loss: 0.201505\n",
      "\u001b[92m[-] Epoch 11/30\tTrain loss: 0.19 \tacc: 93.31(%)\tfm: 81.30(%)\tfw: 93.23(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 11/30\tVal loss: 0.42 \tacc: 89.08(%)\tfm: 63.56(%)\tfw: 89.83(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5619860053133207->0.6355959496379078)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.215333\n",
      "[-] Batch 100/172\t Loss: 0.175072\n",
      "\u001b[92m[-] Epoch 12/30\tTrain loss: 0.14 \tacc: 95.35(%)\tfm: 87.64(%)\tfw: 95.41(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 12/30\tVal loss: 0.28 \tacc: 90.87(%)\tfm: 66.36(%)\tfw: 91.38(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6355959496379078->0.6635588581993718)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.126893\n",
      "[-] Batch 100/172\t Loss: 0.145577\n",
      "\u001b[92m[-] Epoch 13/30\tTrain loss: 0.13 \tacc: 95.70(%)\tfm: 88.37(%)\tfw: 95.77(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 13/30\tVal loss: 0.33 \tacc: 90.24(%)\tfm: 64.82(%)\tfw: 90.97(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.115350\n",
      "[-] Batch 100/172\t Loss: 0.129749\n",
      "\u001b[92m[-] Epoch 14/30\tTrain loss: 0.10 \tacc: 96.79(%)\tfm: 91.60(%)\tfw: 96.81(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 14/30\tVal loss: 0.29 \tacc: 91.19(%)\tfm: 64.16(%)\tfw: 91.38(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.102656\n",
      "[-] Batch 100/172\t Loss: 0.115665\n",
      "\u001b[92m[-] Epoch 15/30\tTrain loss: 0.09 \tacc: 97.09(%)\tfm: 92.08(%)\tfw: 97.10(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 15/30\tVal loss: 0.30 \tacc: 90.63(%)\tfm: 64.67(%)\tfw: 91.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.076128\n",
      "[-] Batch 100/172\t Loss: 0.111204\n",
      "\u001b[92m[-] Epoch 16/30\tTrain loss: 0.08 \tacc: 97.53(%)\tfm: 93.52(%)\tfw: 97.55(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 16/30\tVal loss: 0.32 \tacc: 90.63(%)\tfm: 66.01(%)\tfw: 91.04(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.100359\n",
      "[-] Batch 100/172\t Loss: 0.093494\n",
      "\u001b[92m[-] Epoch 17/30\tTrain loss: 0.07 \tacc: 97.48(%)\tfm: 93.73(%)\tfw: 97.48(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 17/30\tVal loss: 0.27 \tacc: 92.15(%)\tfm: 67.50(%)\tfw: 92.16(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6635588581993718->0.6750163784060095)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.118854\n",
      "[-] Batch 100/172\t Loss: 0.087389\n",
      "\u001b[92m[-] Epoch 18/30\tTrain loss: 0.07 \tacc: 97.62(%)\tfm: 93.91(%)\tfw: 97.64(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 18/30\tVal loss: 0.35 \tacc: 90.63(%)\tfm: 64.85(%)\tfw: 91.15(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.106602\n",
      "[-] Batch 100/172\t Loss: 0.080531\n",
      "\u001b[92m[-] Epoch 19/30\tTrain loss: 0.05 \tacc: 98.19(%)\tfm: 95.96(%)\tfw: 98.18(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 19/30\tVal loss: 0.37 \tacc: 90.71(%)\tfm: 68.18(%)\tfw: 90.74(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6750163784060095->0.6818303117878617)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.081146\n",
      "[-] Batch 100/172\t Loss: 0.071790\n",
      "\u001b[92m[-] Epoch 20/30\tTrain loss: 0.06 \tacc: 98.07(%)\tfm: 95.48(%)\tfw: 98.10(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 20/30\tVal loss: 0.40 \tacc: 90.12(%)\tfm: 66.15(%)\tfw: 90.69(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.027921\n",
      "[-] Batch 100/172\t Loss: 0.061068\n",
      "\u001b[92m[-] Epoch 21/30\tTrain loss: 0.04 \tacc: 98.75(%)\tfm: 97.11(%)\tfw: 98.75(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 21/30\tVal loss: 0.42 \tacc: 90.04(%)\tfm: 63.62(%)\tfw: 90.56(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.087909\n",
      "[-] Batch 100/172\t Loss: 0.057137\n",
      "\u001b[92m[-] Epoch 22/30\tTrain loss: 0.04 \tacc: 98.80(%)\tfm: 96.96(%)\tfw: 98.81(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 22/30\tVal loss: 0.34 \tacc: 91.31(%)\tfm: 66.51(%)\tfw: 91.60(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.027433\n",
      "[-] Batch 100/172\t Loss: 0.044593\n",
      "\u001b[92m[-] Epoch 23/30\tTrain loss: 0.05 \tacc: 98.35(%)\tfm: 96.60(%)\tfw: 98.38(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 23/30\tVal loss: 0.41 \tacc: 89.64(%)\tfm: 65.60(%)\tfw: 90.53(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.086346\n",
      "[-] Batch 100/172\t Loss: 0.044696\n",
      "\u001b[92m[-] Epoch 24/30\tTrain loss: 0.03 \tacc: 99.10(%)\tfm: 97.94(%)\tfw: 99.10(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 24/30\tVal loss: 0.44 \tacc: 89.72(%)\tfm: 63.93(%)\tfw: 90.09(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.049004\n",
      "[-] Batch 100/172\t Loss: 0.046047\n",
      "\u001b[92m[-] Epoch 25/30\tTrain loss: 0.02 \tacc: 99.28(%)\tfm: 98.33(%)\tfw: 99.28(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 25/30\tVal loss: 0.32 \tacc: 93.30(%)\tfm: 70.87(%)\tfw: 93.42(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6818303117878617->0.7086595324969986)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.043721\n",
      "[-] Batch 100/172\t Loss: 0.036737\n",
      "\u001b[92m[-] Epoch 26/30\tTrain loss: 0.03 \tacc: 99.14(%)\tfm: 98.41(%)\tfw: 99.14(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 26/30\tVal loss: 0.42 \tacc: 90.67(%)\tfm: 68.45(%)\tfw: 91.19(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.029518\n",
      "[-] Batch 100/172\t Loss: 0.033984\n",
      "\u001b[92m[-] Epoch 27/30\tTrain loss: 0.03 \tacc: 99.10(%)\tfm: 97.87(%)\tfw: 99.10(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 27/30\tVal loss: 0.46 \tacc: 90.31(%)\tfm: 62.26(%)\tfw: 90.36(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.040264\n",
      "[-] Batch 100/172\t Loss: 0.035934\n",
      "\u001b[92m[-] Epoch 28/30\tTrain loss: 0.03 \tacc: 99.15(%)\tfm: 98.06(%)\tfw: 99.15(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 28/30\tVal loss: 0.40 \tacc: 90.79(%)\tfm: 67.12(%)\tfw: 91.35(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.048019\n",
      "[-] Batch 100/172\t Loss: 0.033076\n",
      "\u001b[92m[-] Epoch 29/30\tTrain loss: 0.03 \tacc: 99.05(%)\tfm: 97.82(%)\tfw: 99.05(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 29/30\tVal loss: 0.38 \tacc: 91.23(%)\tfm: 69.49(%)\tfw: 91.70(%)\u001b[0m\n",
      "\u001b[92mFinished HAR training loop (h:m:s): 0:34:39\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from DeepConvLSTM_py3 import model_train\n",
    "\n",
    "# Define train config options\n",
    "config_train = {'batch_size': 256,\n",
    "                'optimizer': 'Adam',\n",
    "                'lr': 1e-3,\n",
    "                'lr_step': 10,\n",
    "                'lr_decay': 0.9,\n",
    "                'init_weights': 'orthogonal',\n",
    "                'epochs': 30,\n",
    "                'print_freq': 100\n",
    "               }\n",
    "\n",
    "model_train(deepconv, dataset, dataset_val, config_train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data to evaluate the trained model, and setup the test configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mCreating opportunity test HAR dataset of size 118726 ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset_test = SensorDataset(**config_dataset, prefix=\"test\")\n",
    "test_config = {'batch_size': 256,\n",
    "              'train_mode': False,\n",
    "              'dataset': target_dataset,\n",
    "              'num_batches_eval': 212}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model_eval and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mRunning HAR evaluation loop ...\u001b[0m\n",
      "[-] Loading checkpoint ...\n",
      "\u001b[92m[-] Test loss: 0.52\tacc: 89.20(%)\tfm: 58.28(%)\tfw: 89.00(%)\u001b[0m\n",
      "\u001b[92m[Finished HAR evaluation loop (h:m:s): 0:01:18\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from DeepConvLSTM_py3 import model_eval\n",
    "model_eval(deepconv, dataset_test, test_config, return_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use [thop](https://pypi.org/project/thop/) to count the number of floating point operations performed during a forward pass of one batch of synthetic data. We divide by the batch size to get the number of operations per window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_lstm() for <class 'torch.nn.modules.rnn.LSTM'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'DeepConvLSTM_py3.DeepConvLSTM'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "Number of floating point operations per forward pass of one sliding window segment: 115,671,040.0\n"
     ]
    }
   ],
   "source": [
    "# Get number of flops\n",
    "from thop import profile\n",
    "import torch\n",
    "deepconv = deepconv.train()\n",
    "\n",
    "x = torch.ones([config_train['batch_size'], config_dataset['window'], n_channels]).cuda()\n",
    "macs, params = profile(deepconv, inputs=(x,), verbose=True)\n",
    "flops = macs / config_train['batch_size']\n",
    "print(f'Number of floating point operations per forward pass of one sliding window segment: {flops:,}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
