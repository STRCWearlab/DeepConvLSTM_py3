{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Opportunity challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and preprocess data\n",
    "\n",
    "First, download the dataset for the opportunity challenge and set up the directories to hold the raw and processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-29 12:50:26--  https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 306636009 (292M) [application/x-httpd-php]\n",
      "Saving to: ‘OpportunityUCIDataset.zip’\n",
      "\n",
      "OpportunityUCIDatas 100%[===================>] 292.43M  8.53MB/s    in 66s     \n",
      "\n",
      "2021-09-29 12:51:33 (4.44 MB/s) - ‘OpportunityUCIDataset.zip’ saved [306636009/306636009]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\n",
    "! mkdir -p data/raw\n",
    "! mv OpportunityUCIDataset.zip data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following script to preprocess the data. We collate all the training data into one file, same for the validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset data/raw/OpportunityUCIDataset.zip\n",
      "Processing dataset files ...\n",
      "Generating training files\n",
      "... file OpportunityUCIDataset/dataset/S1-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL3.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL4.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-Drill.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL1.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL2.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL3.dat -> train_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL3.dat -> train_data\n",
      "Generating validation files\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL5.dat -> val_data\n",
      "Generating testing files\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL4.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL5.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL4.dat -> test_data\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL5.dat -> test_data\n",
      "[*] Reading raw files from data/opportunity\n",
      "[-] Train data : (43985, 24, 113) float32, target (43985,) int64\n",
      "[-] Valid data : (2509, 24, 113) float32, target (2509,) int64\n",
      "[-] Test data : (9894, 24, 113) float32, target (9894,) int64\n",
      "[-] Test data sample-wise : (118726, 24, 113) float32, target sample-wise (118726,) int64\n",
      "[+] Processed segment datasets successfully saved!\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 preprocess_opportunity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Config\n",
    "\n",
    "We choose the parameters of our sliding window, and specify the name and location of the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = 'opportunity'\n",
    "window_size = 24\n",
    "window_step = 12\n",
    "n_classes = 18\n",
    "\n",
    "config_dataset = {\n",
    "        \"dataset\": target_dataset,\n",
    "        \"window\": window_size,\n",
    "        \"stride\": window_step,\n",
    "        \"stride_test\": 1,\n",
    "        \"path_processed\": f\"data/{target_dataset}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data for training and validation (see datasets.py for sensor dataset implementation). We get the number of channels from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mCreating opportunity train HAR dataset of size 43985 ...\u001b[0m\n",
      "\u001b[92mCreating opportunity val HAR dataset of size 2509 ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import SensorDataset\n",
    "dataset = SensorDataset(**config_dataset, prefix=\"train\")\n",
    "dataset_val = SensorDataset(**config_dataset, prefix=\"val\")\n",
    "n_channels = dataset.n_channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import DeepConvLSTM class\n",
    "\n",
    "See DeepConvLSTM_py3.py for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepConvLSTM_py3 import DeepConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an instance of DeepConvLSTM, with the number of channels and classes defined earlier. \n",
    "The dataset arg determines where the results and training checkpoints should be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepconv = DeepConvLSTM(n_channels=n_channels, n_classes=n_classes, dataset=target_dataset).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the model for 300 epochs. We use a learning rate scheduler to decrease the maximum learning rate for all parameters every 10 epochs, by a factor of 0.9. Check model_train docstring for explanation of the config keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mRunning HAR training loop ...\u001b[0m\n",
      "\u001b[92m[-] Initializing weights (orthogonal)...\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 2.890290\n",
      "[-] Batch 100/172\t Loss: 1.374507\n",
      "\u001b[92m[-] Epoch 0/300\tTrain loss: 0.99 \tacc: 73.92(%)\tfm: 12.87(%)\tfw: 68.17(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 0/300\tVal loss: 0.63 \tacc: 83.30(%)\tfm: 10.82(%)\tfw: 80.32(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.0->0.10818871616427658)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.973741\n",
      "[-] Batch 100/172\t Loss: 0.880867\n",
      "\u001b[92m[-] Epoch 1/300\tTrain loss: 0.63 \tacc: 80.04(%)\tfm: 33.59(%)\tfw: 75.72(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 1/300\tVal loss: 0.47 \tacc: 84.02(%)\tfm: 21.69(%)\tfw: 82.00(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.10818871616427658->0.216920304039408)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.600034\n",
      "[-] Batch 100/172\t Loss: 0.590870\n",
      "\u001b[92m[-] Epoch 2/300\tTrain loss: 0.49 \tacc: 82.56(%)\tfm: 44.05(%)\tfw: 80.68(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 2/300\tVal loss: 0.36 \tacc: 86.57(%)\tfm: 35.89(%)\tfw: 85.87(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.216920304039408->0.3588586928844317)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.439499\n",
      "[-] Batch 100/172\t Loss: 0.478449\n",
      "\u001b[92m[-] Epoch 3/300\tTrain loss: 0.39 \tacc: 86.16(%)\tfm: 55.86(%)\tfw: 85.21(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 3/300\tVal loss: 0.37 \tacc: 87.21(%)\tfm: 53.02(%)\tfw: 87.63(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.3588586928844317->0.5302003406483196)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.362705\n",
      "[-] Batch 100/172\t Loss: 0.390207\n",
      "\u001b[92m[-] Epoch 4/300\tTrain loss: 0.34 \tacc: 88.16(%)\tfm: 63.54(%)\tfw: 87.01(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 4/300\tVal loss: 0.32 \tacc: 88.00(%)\tfm: 48.89(%)\tfw: 87.15(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.314681\n",
      "[-] Batch 100/172\t Loss: 0.322884\n",
      "\u001b[92m[-] Epoch 5/300\tTrain loss: 0.28 \tacc: 90.06(%)\tfm: 71.83(%)\tfw: 89.54(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 5/300\tVal loss: 0.34 \tacc: 87.80(%)\tfm: 54.63(%)\tfw: 87.57(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5302003406483196->0.5463332210584086)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.258409\n",
      "[-] Batch 100/172\t Loss: 0.286751\n",
      "\u001b[92m[-] Epoch 6/300\tTrain loss: 0.24 \tacc: 91.69(%)\tfm: 76.82(%)\tfw: 91.55(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 6/300\tVal loss: 0.37 \tacc: 88.08(%)\tfm: 58.97(%)\tfw: 88.76(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5463332210584086->0.5896618447362422)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.288221\n",
      "[-] Batch 100/172\t Loss: 0.239444\n",
      "\u001b[92m[-] Epoch 7/300\tTrain loss: 0.21 \tacc: 92.88(%)\tfm: 81.15(%)\tfw: 92.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 7/300\tVal loss: 0.40 \tacc: 88.08(%)\tfm: 65.43(%)\tfw: 89.29(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.5896618447362422->0.6543161799402308)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.237374\n",
      "[-] Batch 100/172\t Loss: 0.215030\n",
      "\u001b[92m[-] Epoch 8/300\tTrain loss: 0.18 \tacc: 93.73(%)\tfm: 84.73(%)\tfw: 93.67(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 8/300\tVal loss: 0.46 \tacc: 87.41(%)\tfm: 58.57(%)\tfw: 88.05(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.001\n",
      "[-] Batch 0/172\t Loss: 0.245399\n",
      "[-] Batch 100/172\t Loss: 0.189782\n",
      "\u001b[92m[-] Epoch 9/300\tTrain loss: 0.14 \tacc: 95.36(%)\tfm: 88.17(%)\tfw: 95.32(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 9/300\tVal loss: 0.32 \tacc: 90.35(%)\tfm: 62.84(%)\tfw: 90.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.194481\n",
      "[-] Batch 100/172\t Loss: 0.160515\n",
      "\u001b[92m[-] Epoch 10/300\tTrain loss: 0.13 \tacc: 95.83(%)\tfm: 89.69(%)\tfw: 95.82(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 10/300\tVal loss: 0.38 \tacc: 88.80(%)\tfm: 60.63(%)\tfw: 89.08(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.100857\n",
      "[-] Batch 100/172\t Loss: 0.143341\n",
      "\u001b[92m[-] Epoch 11/300\tTrain loss: 0.11 \tacc: 96.50(%)\tfm: 91.54(%)\tfw: 96.46(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 11/300\tVal loss: 0.37 \tacc: 89.36(%)\tfm: 62.46(%)\tfw: 89.55(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.150575\n",
      "[-] Batch 100/172\t Loss: 0.120636\n",
      "\u001b[92m[-] Epoch 12/300\tTrain loss: 0.09 \tacc: 96.95(%)\tfm: 92.90(%)\tfw: 96.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 12/300\tVal loss: 0.33 \tacc: 89.92(%)\tfm: 65.72(%)\tfw: 90.29(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6543161799402308->0.6571880828205516)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.110100\n",
      "[-] Batch 100/172\t Loss: 0.106084\n",
      "\u001b[92m[-] Epoch 13/300\tTrain loss: 0.08 \tacc: 97.33(%)\tfm: 93.48(%)\tfw: 97.32(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 13/300\tVal loss: 0.36 \tacc: 89.52(%)\tfm: 64.82(%)\tfw: 90.06(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.126962\n",
      "[-] Batch 100/172\t Loss: 0.101323\n",
      "\u001b[92m[-] Epoch 14/300\tTrain loss: 0.07 \tacc: 97.63(%)\tfm: 94.75(%)\tfw: 97.65(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 14/300\tVal loss: 0.40 \tacc: 89.08(%)\tfm: 70.04(%)\tfw: 90.00(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.6571880828205516->0.7003701206803902)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.111750\n",
      "[-] Batch 100/172\t Loss: 0.092088\n",
      "\u001b[92m[-] Epoch 15/300\tTrain loss: 0.07 \tacc: 97.52(%)\tfm: 94.57(%)\tfw: 97.55(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 15/300\tVal loss: 0.47 \tacc: 88.44(%)\tfm: 66.98(%)\tfw: 89.52(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.086433\n",
      "[-] Batch 100/172\t Loss: 0.083129\n",
      "\u001b[92m[-] Epoch 16/300\tTrain loss: 0.07 \tacc: 97.54(%)\tfm: 94.66(%)\tfw: 97.58(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 16/300\tVal loss: 0.51 \tacc: 88.04(%)\tfm: 62.27(%)\tfw: 88.91(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.064497\n",
      "[-] Batch 100/172\t Loss: 0.071153\n",
      "\u001b[92m[-] Epoch 17/300\tTrain loss: 0.06 \tacc: 98.11(%)\tfm: 95.53(%)\tfw: 98.13(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 17/300\tVal loss: 0.43 \tacc: 89.28(%)\tfm: 62.77(%)\tfw: 89.77(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.060832\n",
      "[-] Batch 100/172\t Loss: 0.069411\n",
      "\u001b[92m[-] Epoch 18/300\tTrain loss: 0.05 \tacc: 98.24(%)\tfm: 95.87(%)\tfw: 98.24(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 18/300\tVal loss: 0.37 \tacc: 90.47(%)\tfm: 68.15(%)\tfw: 90.94(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0009000000000000001\n",
      "[-] Batch 0/172\t Loss: 0.037628\n",
      "[-] Batch 100/172\t Loss: 0.054911\n",
      "\u001b[92m[-] Epoch 19/300\tTrain loss: 0.06 \tacc: 98.17(%)\tfm: 95.71(%)\tfw: 98.19(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 19/300\tVal loss: 0.50 \tacc: 89.04(%)\tfm: 67.49(%)\tfw: 89.91(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.039126\n",
      "[-] Batch 100/172\t Loss: 0.055618\n",
      "\u001b[92m[-] Epoch 20/300\tTrain loss: 0.04 \tacc: 98.70(%)\tfm: 97.01(%)\tfw: 98.70(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 20/300\tVal loss: 0.47 \tacc: 89.80(%)\tfm: 66.76(%)\tfw: 90.34(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.019057\n",
      "[-] Batch 100/172\t Loss: 0.050804\n",
      "\u001b[92m[-] Epoch 21/300\tTrain loss: 0.03 \tacc: 98.88(%)\tfm: 97.76(%)\tfw: 98.89(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 21/300\tVal loss: 0.38 \tacc: 91.07(%)\tfm: 70.13(%)\tfw: 91.38(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7003701206803902->0.7013439130643891)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.054899\n",
      "[-] Batch 100/172\t Loss: 0.039912\n",
      "\u001b[92m[-] Epoch 22/300\tTrain loss: 0.04 \tacc: 98.80(%)\tfm: 97.43(%)\tfw: 98.81(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 22/300\tVal loss: 0.47 \tacc: 89.52(%)\tfm: 69.70(%)\tfw: 90.33(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.060328\n",
      "[-] Batch 100/172\t Loss: 0.042757\n",
      "\u001b[92m[-] Epoch 23/300\tTrain loss: 0.04 \tacc: 98.79(%)\tfm: 97.14(%)\tfw: 98.80(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 23/300\tVal loss: 0.45 \tacc: 89.00(%)\tfm: 66.57(%)\tfw: 89.84(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.043571\n",
      "[-] Batch 100/172\t Loss: 0.041288\n",
      "\u001b[92m[-] Epoch 24/300\tTrain loss: 0.03 \tacc: 98.89(%)\tfm: 97.21(%)\tfw: 98.89(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 24/300\tVal loss: 0.41 \tacc: 90.87(%)\tfm: 63.59(%)\tfw: 90.72(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.030371\n",
      "[-] Batch 100/172\t Loss: 0.042043\n",
      "\u001b[92m[-] Epoch 25/300\tTrain loss: 0.03 \tacc: 99.11(%)\tfm: 97.93(%)\tfw: 99.11(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 25/300\tVal loss: 0.43 \tacc: 90.51(%)\tfm: 70.52(%)\tfw: 91.01(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7013439130643891->0.705217097093312)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.037116\n",
      "[-] Batch 100/172\t Loss: 0.034665\n",
      "\u001b[92m[-] Epoch 26/300\tTrain loss: 0.02 \tacc: 99.25(%)\tfm: 98.25(%)\tfw: 99.25(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 26/300\tVal loss: 0.45 \tacc: 89.12(%)\tfm: 62.39(%)\tfw: 89.42(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.028335\n",
      "[-] Batch 100/172\t Loss: 0.032923\n",
      "\u001b[92m[-] Epoch 27/300\tTrain loss: 0.02 \tacc: 99.30(%)\tfm: 98.37(%)\tfw: 99.31(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 27/300\tVal loss: 0.46 \tacc: 89.80(%)\tfm: 66.01(%)\tfw: 90.32(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.041533\n",
      "[-] Batch 100/172\t Loss: 0.031496\n",
      "\u001b[92m[-] Epoch 28/300\tTrain loss: 0.02 \tacc: 99.26(%)\tfm: 98.42(%)\tfw: 99.26(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 28/300\tVal loss: 0.45 \tacc: 90.47(%)\tfm: 67.49(%)\tfw: 91.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0008100000000000001\n",
      "[-] Batch 0/172\t Loss: 0.019446\n",
      "[-] Batch 100/172\t Loss: 0.032222\n",
      "\u001b[92m[-] Epoch 29/300\tTrain loss: 0.03 \tacc: 99.17(%)\tfm: 97.48(%)\tfw: 99.16(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 29/300\tVal loss: 0.48 \tacc: 90.75(%)\tfm: 63.42(%)\tfw: 90.90(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.040579\n",
      "[-] Batch 100/172\t Loss: 0.028650\n",
      "\u001b[92m[-] Epoch 30/300\tTrain loss: 0.02 \tacc: 99.43(%)\tfm: 98.82(%)\tfw: 99.43(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 30/300\tVal loss: 0.40 \tacc: 91.23(%)\tfm: 66.98(%)\tfw: 91.41(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.013173\n",
      "[-] Batch 100/172\t Loss: 0.023895\n",
      "\u001b[92m[-] Epoch 31/300\tTrain loss: 0.02 \tacc: 99.32(%)\tfm: 98.84(%)\tfw: 99.33(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 31/300\tVal loss: 0.59 \tacc: 88.72(%)\tfm: 63.14(%)\tfw: 89.37(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.020152\n",
      "[-] Batch 100/172\t Loss: 0.022323\n",
      "\u001b[92m[-] Epoch 32/300\tTrain loss: 0.03 \tacc: 99.03(%)\tfm: 98.94(%)\tfw: 99.04(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 32/300\tVal loss: 0.67 \tacc: 88.24(%)\tfm: 69.44(%)\tfw: 89.43(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.039738\n",
      "[-] Batch 100/172\t Loss: 0.031318\n",
      "\u001b[92m[-] Epoch 33/300\tTrain loss: 0.02 \tacc: 99.30(%)\tfm: 98.56(%)\tfw: 99.31(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 33/300\tVal loss: 0.56 \tacc: 89.20(%)\tfm: 67.05(%)\tfw: 89.97(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.064188\n",
      "[-] Batch 100/172\t Loss: 0.024864\n",
      "\u001b[92m[-] Epoch 34/300\tTrain loss: 0.02 \tacc: 99.37(%)\tfm: 98.71(%)\tfw: 99.38(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 34/300\tVal loss: 0.58 \tacc: 89.28(%)\tfm: 66.13(%)\tfw: 90.09(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.031025\n",
      "[-] Batch 100/172\t Loss: 0.025668\n",
      "\u001b[92m[-] Epoch 35/300\tTrain loss: 0.02 \tacc: 99.23(%)\tfm: 98.66(%)\tfw: 99.23(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 35/300\tVal loss: 0.63 \tacc: 88.48(%)\tfm: 64.73(%)\tfw: 89.48(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.024274\n",
      "[-] Batch 100/172\t Loss: 0.022470\n",
      "\u001b[92m[-] Epoch 36/300\tTrain loss: 0.01 \tacc: 99.53(%)\tfm: 99.01(%)\tfw: 99.53(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 36/300\tVal loss: 0.53 \tacc: 89.44(%)\tfm: 67.44(%)\tfw: 90.06(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.014075\n",
      "[-] Batch 100/172\t Loss: 0.025490\n",
      "\u001b[92m[-] Epoch 37/300\tTrain loss: 0.01 \tacc: 99.52(%)\tfm: 98.80(%)\tfw: 99.52(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 37/300\tVal loss: 0.51 \tacc: 90.55(%)\tfm: 64.88(%)\tfw: 90.60(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.018174\n",
      "[-] Batch 100/172\t Loss: 0.024262\n",
      "\u001b[92m[-] Epoch 38/300\tTrain loss: 0.01 \tacc: 99.56(%)\tfm: 99.02(%)\tfw: 99.56(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 38/300\tVal loss: 0.53 \tacc: 90.20(%)\tfm: 67.52(%)\tfw: 90.55(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000729\n",
      "[-] Batch 0/172\t Loss: 0.017540\n",
      "[-] Batch 100/172\t Loss: 0.022759\n",
      "\u001b[92m[-] Epoch 39/300\tTrain loss: 0.01 \tacc: 99.48(%)\tfm: 99.30(%)\tfw: 99.48(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 39/300\tVal loss: 0.64 \tacc: 88.88(%)\tfm: 67.77(%)\tfw: 89.64(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.030696\n",
      "[-] Batch 100/172\t Loss: 0.015526\n",
      "\u001b[92m[-] Epoch 40/300\tTrain loss: 0.01 \tacc: 99.76(%)\tfm: 99.46(%)\tfw: 99.76(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 40/300\tVal loss: 0.53 \tacc: 90.35(%)\tfm: 65.08(%)\tfw: 90.60(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.003304\n",
      "[-] Batch 100/172\t Loss: 0.017094\n",
      "\u001b[92m[-] Epoch 41/300\tTrain loss: 0.01 \tacc: 99.55(%)\tfm: 99.08(%)\tfw: 99.55(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 41/300\tVal loss: 0.63 \tacc: 89.12(%)\tfm: 69.50(%)\tfw: 90.01(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.004072\n",
      "[-] Batch 100/172\t Loss: 0.015108\n",
      "\u001b[92m[-] Epoch 42/300\tTrain loss: 0.01 \tacc: 99.76(%)\tfm: 99.48(%)\tfw: 99.76(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 42/300\tVal loss: 0.46 \tacc: 91.43(%)\tfm: 70.25(%)\tfw: 91.62(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.016636\n",
      "[-] Batch 100/172\t Loss: 0.021700\n",
      "\u001b[92m[-] Epoch 43/300\tTrain loss: 0.01 \tacc: 99.73(%)\tfm: 99.46(%)\tfw: 99.73(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 43/300\tVal loss: 0.57 \tacc: 90.16(%)\tfm: 69.66(%)\tfw: 90.72(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.012226\n",
      "[-] Batch 100/172\t Loss: 0.013615\n",
      "\u001b[92m[-] Epoch 44/300\tTrain loss: 0.01 \tacc: 99.79(%)\tfm: 99.50(%)\tfw: 99.79(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 44/300\tVal loss: 0.53 \tacc: 90.12(%)\tfm: 69.60(%)\tfw: 90.83(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.002566\n",
      "[-] Batch 100/172\t Loss: 0.012599\n",
      "\u001b[92m[-] Epoch 45/300\tTrain loss: 0.01 \tacc: 99.85(%)\tfm: 99.71(%)\tfw: 99.85(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 45/300\tVal loss: 0.54 \tacc: 90.83(%)\tfm: 69.86(%)\tfw: 91.28(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.006807\n",
      "[-] Batch 100/172\t Loss: 0.015406\n",
      "\u001b[92m[-] Epoch 46/300\tTrain loss: 0.01 \tacc: 99.77(%)\tfm: 99.43(%)\tfw: 99.77(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 46/300\tVal loss: 0.55 \tacc: 90.20(%)\tfm: 69.66(%)\tfw: 90.74(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.004170\n",
      "[-] Batch 100/172\t Loss: 0.013049\n",
      "\u001b[92m[-] Epoch 47/300\tTrain loss: 0.01 \tacc: 99.75(%)\tfm: 99.41(%)\tfw: 99.75(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 47/300\tVal loss: 0.49 \tacc: 90.55(%)\tfm: 67.86(%)\tfw: 90.82(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.026533\n",
      "[-] Batch 100/172\t Loss: 0.012693\n",
      "\u001b[92m[-] Epoch 48/300\tTrain loss: 0.01 \tacc: 99.62(%)\tfm: 99.06(%)\tfw: 99.62(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 48/300\tVal loss: 0.54 \tacc: 91.03(%)\tfm: 70.62(%)\tfw: 91.20(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.705217097093312->0.7061816788101576)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0006561000000000001\n",
      "[-] Batch 0/172\t Loss: 0.019182\n",
      "[-] Batch 100/172\t Loss: 0.020470\n",
      "\u001b[92m[-] Epoch 49/300\tTrain loss: 0.01 \tacc: 99.68(%)\tfm: 99.28(%)\tfw: 99.68(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 49/300\tVal loss: 0.56 \tacc: 90.43(%)\tfm: 67.98(%)\tfw: 90.80(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.009768\n",
      "[-] Batch 100/172\t Loss: 0.013157\n",
      "\u001b[92m[-] Epoch 50/300\tTrain loss: 0.01 \tacc: 99.82(%)\tfm: 99.71(%)\tfw: 99.82(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 50/300\tVal loss: 0.55 \tacc: 89.88(%)\tfm: 65.87(%)\tfw: 90.37(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.009184\n",
      "[-] Batch 100/172\t Loss: 0.010038\n",
      "\u001b[92m[-] Epoch 51/300\tTrain loss: 0.01 \tacc: 99.80(%)\tfm: 99.51(%)\tfw: 99.80(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 51/300\tVal loss: 0.55 \tacc: 90.83(%)\tfm: 71.42(%)\tfw: 91.33(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7061816788101576->0.7142345613817969)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.013029\n",
      "[-] Batch 100/172\t Loss: 0.010977\n",
      "\u001b[92m[-] Epoch 52/300\tTrain loss: 0.01 \tacc: 99.85(%)\tfm: 99.73(%)\tfw: 99.85(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 52/300\tVal loss: 0.55 \tacc: 90.47(%)\tfm: 69.36(%)\tfw: 90.90(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.002443\n",
      "[-] Batch 100/172\t Loss: 0.012849\n",
      "\u001b[92m[-] Epoch 53/300\tTrain loss: 0.01 \tacc: 99.84(%)\tfm: 99.62(%)\tfw: 99.84(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 53/300\tVal loss: 0.57 \tacc: 90.55(%)\tfm: 69.10(%)\tfw: 90.99(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.002163\n",
      "[-] Batch 100/172\t Loss: 0.008527\n",
      "\u001b[92m[-] Epoch 54/300\tTrain loss: 0.00 \tacc: 99.92(%)\tfm: 99.81(%)\tfw: 99.92(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 54/300\tVal loss: 0.61 \tacc: 90.47(%)\tfm: 67.96(%)\tfw: 90.82(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.002379\n",
      "[-] Batch 100/172\t Loss: 0.010529\n",
      "\u001b[92m[-] Epoch 55/300\tTrain loss: 0.01 \tacc: 99.77(%)\tfm: 99.54(%)\tfw: 99.77(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 55/300\tVal loss: 0.55 \tacc: 91.63(%)\tfm: 68.68(%)\tfw: 91.79(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.002136\n",
      "[-] Batch 100/172\t Loss: 0.010440\n",
      "\u001b[92m[-] Epoch 56/300\tTrain loss: 0.01 \tacc: 99.79(%)\tfm: 99.55(%)\tfw: 99.79(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 56/300\tVal loss: 0.56 \tacc: 90.31(%)\tfm: 69.82(%)\tfw: 90.93(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.017283\n",
      "[-] Batch 100/172\t Loss: 0.010211\n",
      "\u001b[92m[-] Epoch 57/300\tTrain loss: 0.01 \tacc: 99.81(%)\tfm: 99.62(%)\tfw: 99.81(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 57/300\tVal loss: 0.67 \tacc: 89.96(%)\tfm: 69.15(%)\tfw: 90.60(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.008812\n",
      "[-] Batch 100/172\t Loss: 0.009864\n",
      "\u001b[92m[-] Epoch 58/300\tTrain loss: 0.00 \tacc: 99.87(%)\tfm: 99.66(%)\tfw: 99.87(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 58/300\tVal loss: 0.62 \tacc: 90.12(%)\tfm: 68.61(%)\tfw: 90.70(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00059049\n",
      "[-] Batch 0/172\t Loss: 0.001680\n",
      "[-] Batch 100/172\t Loss: 0.011106\n",
      "\u001b[92m[-] Epoch 59/300\tTrain loss: 0.01 \tacc: 99.84(%)\tfm: 99.68(%)\tfw: 99.84(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 59/300\tVal loss: 0.64 \tacc: 89.84(%)\tfm: 67.81(%)\tfw: 90.50(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.014748\n",
      "[-] Batch 100/172\t Loss: 0.009781\n",
      "\u001b[92m[-] Epoch 60/300\tTrain loss: 0.00 \tacc: 99.86(%)\tfm: 99.69(%)\tfw: 99.86(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 60/300\tVal loss: 0.52 \tacc: 91.35(%)\tfm: 70.77(%)\tfw: 91.69(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.001909\n",
      "[-] Batch 100/172\t Loss: 0.007464\n",
      "\u001b[92m[-] Epoch 61/300\tTrain loss: 0.01 \tacc: 99.85(%)\tfm: 99.68(%)\tfw: 99.85(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 61/300\tVal loss: 0.61 \tacc: 89.72(%)\tfm: 67.81(%)\tfw: 90.50(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.022380\n",
      "[-] Batch 100/172\t Loss: 0.008227\n",
      "\u001b[92m[-] Epoch 62/300\tTrain loss: 0.00 \tacc: 99.92(%)\tfm: 99.90(%)\tfw: 99.92(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 62/300\tVal loss: 0.65 \tacc: 89.84(%)\tfm: 70.24(%)\tfw: 90.59(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.001442\n",
      "[-] Batch 100/172\t Loss: 0.007209\n",
      "\u001b[92m[-] Epoch 63/300\tTrain loss: 0.00 \tacc: 99.90(%)\tfm: 99.82(%)\tfw: 99.90(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 63/300\tVal loss: 0.61 \tacc: 90.31(%)\tfm: 69.89(%)\tfw: 90.83(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.005456\n",
      "[-] Batch 100/172\t Loss: 0.009100\n",
      "\u001b[92m[-] Epoch 64/300\tTrain loss: 0.00 \tacc: 99.90(%)\tfm: 99.83(%)\tfw: 99.90(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 64/300\tVal loss: 0.52 \tacc: 91.51(%)\tfm: 71.94(%)\tfw: 91.74(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7142345613817969->0.7193598794044695)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.001767\n",
      "[-] Batch 100/172\t Loss: 0.007210\n",
      "\u001b[92m[-] Epoch 65/300\tTrain loss: 0.00 \tacc: 99.91(%)\tfm: 99.80(%)\tfw: 99.91(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 65/300\tVal loss: 0.67 \tacc: 89.88(%)\tfm: 67.18(%)\tfw: 90.31(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.004448\n",
      "[-] Batch 100/172\t Loss: 0.005850\n",
      "\u001b[92m[-] Epoch 66/300\tTrain loss: 0.00 \tacc: 99.87(%)\tfm: 99.59(%)\tfw: 99.87(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 66/300\tVal loss: 0.58 \tacc: 90.35(%)\tfm: 67.72(%)\tfw: 90.69(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.008702\n",
      "[-] Batch 100/172\t Loss: 0.007934\n",
      "\u001b[92m[-] Epoch 67/300\tTrain loss: 0.01 \tacc: 99.79(%)\tfm: 99.60(%)\tfw: 99.79(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 67/300\tVal loss: 0.59 \tacc: 89.88(%)\tfm: 66.26(%)\tfw: 90.45(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.010289\n",
      "[-] Batch 100/172\t Loss: 0.007887\n",
      "\u001b[92m[-] Epoch 68/300\tTrain loss: 0.01 \tacc: 99.79(%)\tfm: 99.59(%)\tfw: 99.79(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 68/300\tVal loss: 0.64 \tacc: 89.52(%)\tfm: 67.78(%)\tfw: 90.30(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000531441\n",
      "[-] Batch 0/172\t Loss: 0.006373\n",
      "[-] Batch 100/172\t Loss: 0.009563\n",
      "\u001b[92m[-] Epoch 69/300\tTrain loss: 0.00 \tacc: 99.88(%)\tfm: 99.87(%)\tfw: 99.88(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 69/300\tVal loss: 0.67 \tacc: 89.48(%)\tfm: 68.73(%)\tfw: 90.31(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.004313\n",
      "[-] Batch 100/172\t Loss: 0.005085\n",
      "\u001b[92m[-] Epoch 70/300\tTrain loss: 0.00 \tacc: 99.90(%)\tfm: 99.82(%)\tfw: 99.90(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 70/300\tVal loss: 0.61 \tacc: 90.47(%)\tfm: 70.13(%)\tfw: 91.08(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.004295\n",
      "[-] Batch 100/172\t Loss: 0.006313\n",
      "\u001b[92m[-] Epoch 71/300\tTrain loss: 0.00 \tacc: 99.90(%)\tfm: 99.83(%)\tfw: 99.90(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 71/300\tVal loss: 0.57 \tacc: 91.19(%)\tfm: 70.28(%)\tfw: 91.56(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.003717\n",
      "[-] Batch 100/172\t Loss: 0.008161\n",
      "\u001b[92m[-] Epoch 72/300\tTrain loss: 0.00 \tacc: 99.93(%)\tfm: 99.87(%)\tfw: 99.93(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 72/300\tVal loss: 0.68 \tacc: 89.92(%)\tfm: 68.97(%)\tfw: 90.42(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.003555\n",
      "[-] Batch 100/172\t Loss: 0.004913\n",
      "\u001b[92m[-] Epoch 73/300\tTrain loss: 0.00 \tacc: 99.90(%)\tfm: 99.82(%)\tfw: 99.90(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 73/300\tVal loss: 0.64 \tacc: 90.00(%)\tfm: 70.11(%)\tfw: 90.64(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.013121\n",
      "[-] Batch 100/172\t Loss: 0.006226\n",
      "\u001b[92m[-] Epoch 74/300\tTrain loss: 0.00 \tacc: 99.91(%)\tfm: 99.78(%)\tfw: 99.91(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 74/300\tVal loss: 0.67 \tacc: 89.88(%)\tfm: 69.20(%)\tfw: 90.44(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.000317\n",
      "[-] Batch 100/172\t Loss: 0.005108\n",
      "\u001b[92m[-] Epoch 75/300\tTrain loss: 0.00 \tacc: 99.92(%)\tfm: 99.86(%)\tfw: 99.93(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 75/300\tVal loss: 0.76 \tacc: 89.80(%)\tfm: 68.00(%)\tfw: 90.32(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.001199\n",
      "[-] Batch 100/172\t Loss: 0.007966\n",
      "\u001b[92m[-] Epoch 76/300\tTrain loss: 0.00 \tacc: 99.88(%)\tfm: 99.77(%)\tfw: 99.88(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 76/300\tVal loss: 0.60 \tacc: 90.16(%)\tfm: 68.58(%)\tfw: 90.79(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.012669\n",
      "[-] Batch 100/172\t Loss: 0.004474\n",
      "\u001b[92m[-] Epoch 77/300\tTrain loss: 0.00 \tacc: 99.89(%)\tfm: 99.75(%)\tfw: 99.89(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 77/300\tVal loss: 0.69 \tacc: 89.88(%)\tfm: 68.61(%)\tfw: 90.43(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.002658\n",
      "[-] Batch 100/172\t Loss: 0.006413\n",
      "\u001b[92m[-] Epoch 78/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.93(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 78/300\tVal loss: 0.72 \tacc: 90.35(%)\tfm: 68.47(%)\tfw: 90.81(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0004782969\n",
      "[-] Batch 0/172\t Loss: 0.005813\n",
      "[-] Batch 100/172\t Loss: 0.009209\n",
      "\u001b[92m[-] Epoch 79/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.94(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 79/300\tVal loss: 0.68 \tacc: 90.31(%)\tfm: 70.72(%)\tfw: 90.92(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.000304\n",
      "[-] Batch 100/172\t Loss: 0.005732\n",
      "\u001b[92m[-] Epoch 80/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.93(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 80/300\tVal loss: 0.64 \tacc: 90.67(%)\tfm: 70.91(%)\tfw: 91.17(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.000265\n",
      "[-] Batch 100/172\t Loss: 0.005799\n",
      "\u001b[92m[-] Epoch 81/300\tTrain loss: 0.00 \tacc: 99.88(%)\tfm: 99.60(%)\tfw: 99.88(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 81/300\tVal loss: 0.64 \tacc: 90.51(%)\tfm: 68.70(%)\tfw: 90.98(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.000241\n",
      "[-] Batch 100/172\t Loss: 0.004516\n",
      "\u001b[92m[-] Epoch 82/300\tTrain loss: 0.00 \tacc: 99.85(%)\tfm: 99.77(%)\tfw: 99.85(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 82/300\tVal loss: 0.79 \tacc: 89.44(%)\tfm: 68.70(%)\tfw: 90.24(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.006096\n",
      "[-] Batch 100/172\t Loss: 0.004593\n",
      "\u001b[92m[-] Epoch 83/300\tTrain loss: 0.00 \tacc: 99.88(%)\tfm: 99.75(%)\tfw: 99.88(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 83/300\tVal loss: 0.65 \tacc: 90.16(%)\tfm: 67.91(%)\tfw: 90.78(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.002953\n",
      "[-] Batch 100/172\t Loss: 0.004257\n",
      "\u001b[92m[-] Epoch 84/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.91(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 84/300\tVal loss: 0.62 \tacc: 90.00(%)\tfm: 67.45(%)\tfw: 90.53(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.000793\n",
      "[-] Batch 100/172\t Loss: 0.003137\n",
      "\u001b[92m[-] Epoch 85/300\tTrain loss: 0.00 \tacc: 99.89(%)\tfm: 99.75(%)\tfw: 99.89(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 85/300\tVal loss: 0.61 \tacc: 90.35(%)\tfm: 66.68(%)\tfw: 90.61(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.001335\n",
      "[-] Batch 100/172\t Loss: 0.005340\n",
      "\u001b[92m[-] Epoch 86/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.90(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 86/300\tVal loss: 0.61 \tacc: 90.67(%)\tfm: 66.83(%)\tfw: 90.95(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.012173\n",
      "[-] Batch 100/172\t Loss: 0.004090\n",
      "\u001b[92m[-] Epoch 87/300\tTrain loss: 0.00 \tacc: 99.94(%)\tfm: 99.90(%)\tfw: 99.94(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 87/300\tVal loss: 0.62 \tacc: 90.87(%)\tfm: 69.01(%)\tfw: 91.31(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.000429\n",
      "[-] Batch 100/172\t Loss: 0.003994\n",
      "\u001b[92m[-] Epoch 88/300\tTrain loss: 0.01 \tacc: 99.75(%)\tfm: 99.71(%)\tfw: 99.75(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 88/300\tVal loss: 0.80 \tacc: 88.48(%)\tfm: 69.91(%)\tfw: 89.64(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00043046721\n",
      "[-] Batch 0/172\t Loss: 0.017900\n",
      "[-] Batch 100/172\t Loss: 0.007570\n",
      "\u001b[92m[-] Epoch 89/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.92(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 89/300\tVal loss: 0.62 \tacc: 90.67(%)\tfm: 68.01(%)\tfw: 90.97(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.002446\n",
      "[-] Batch 100/172\t Loss: 0.005614\n",
      "\u001b[92m[-] Epoch 90/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.97(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 90/300\tVal loss: 0.73 \tacc: 89.92(%)\tfm: 69.17(%)\tfw: 90.55(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.000936\n",
      "[-] Batch 100/172\t Loss: 0.004048\n",
      "\u001b[92m[-] Epoch 91/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.94(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 91/300\tVal loss: 0.69 \tacc: 90.51(%)\tfm: 69.39(%)\tfw: 90.96(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.002257\n",
      "[-] Batch 100/172\t Loss: 0.003330\n",
      "\u001b[92m[-] Epoch 92/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.91(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 92/300\tVal loss: 0.68 \tacc: 89.96(%)\tfm: 67.95(%)\tfw: 90.54(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.008397\n",
      "[-] Batch 100/172\t Loss: 0.002252\n",
      "\u001b[92m[-] Epoch 93/300\tTrain loss: 0.00 \tacc: 99.93(%)\tfm: 99.90(%)\tfw: 99.93(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 93/300\tVal loss: 0.69 \tacc: 90.35(%)\tfm: 68.88(%)\tfw: 90.72(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.018398\n",
      "[-] Batch 100/172\t Loss: 0.004338\n",
      "\u001b[92m[-] Epoch 94/300\tTrain loss: 0.00 \tacc: 99.88(%)\tfm: 99.80(%)\tfw: 99.88(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 94/300\tVal loss: 0.70 \tacc: 90.00(%)\tfm: 70.65(%)\tfw: 90.65(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.002939\n",
      "[-] Batch 100/172\t Loss: 0.005410\n",
      "\u001b[92m[-] Epoch 95/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 95/300\tVal loss: 0.59 \tacc: 91.27(%)\tfm: 70.04(%)\tfw: 91.45(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.000282\n",
      "[-] Batch 100/172\t Loss: 0.003784\n",
      "\u001b[92m[-] Epoch 96/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.95(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 96/300\tVal loss: 0.57 \tacc: 91.63(%)\tfm: 68.24(%)\tfw: 91.75(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.033840\n",
      "[-] Batch 100/172\t Loss: 0.003176\n",
      "\u001b[92m[-] Epoch 97/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.91(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 97/300\tVal loss: 0.64 \tacc: 90.43(%)\tfm: 69.27(%)\tfw: 91.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.010409\n",
      "[-] Batch 100/172\t Loss: 0.003876\n",
      "\u001b[92m[-] Epoch 98/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.96(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 98/300\tVal loss: 0.63 \tacc: 90.83(%)\tfm: 69.68(%)\tfw: 91.28(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.000387420489\n",
      "[-] Batch 0/172\t Loss: 0.000180\n",
      "[-] Batch 100/172\t Loss: 0.002016\n",
      "\u001b[92m[-] Epoch 99/300\tTrain loss: 0.00 \tacc: 99.94(%)\tfm: 99.90(%)\tfw: 99.94(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 99/300\tVal loss: 0.76 \tacc: 89.56(%)\tfm: 66.94(%)\tfw: 90.27(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.003578\n",
      "[-] Batch 100/172\t Loss: 0.002591\n",
      "\u001b[92m[-] Epoch 100/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.96(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 100/300\tVal loss: 0.67 \tacc: 90.67(%)\tfm: 68.74(%)\tfw: 90.98(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000306\n",
      "[-] Batch 100/172\t Loss: 0.002841\n",
      "\u001b[92m[-] Epoch 101/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.99(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 101/300\tVal loss: 0.79 \tacc: 89.64(%)\tfm: 67.17(%)\tfw: 90.23(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000332\n",
      "[-] Batch 100/172\t Loss: 0.001291\n",
      "\u001b[92m[-] Epoch 102/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.97(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 102/300\tVal loss: 0.70 \tacc: 90.16(%)\tfm: 69.28(%)\tfw: 90.80(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000142\n",
      "[-] Batch 100/172\t Loss: 0.003435\n",
      "\u001b[92m[-] Epoch 103/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.89(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 103/300\tVal loss: 0.71 \tacc: 90.04(%)\tfm: 64.58(%)\tfw: 90.51(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000410\n",
      "[-] Batch 100/172\t Loss: 0.003721\n",
      "\u001b[92m[-] Epoch 104/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.92(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 104/300\tVal loss: 0.75 \tacc: 89.68(%)\tfm: 65.19(%)\tfw: 90.30(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000242\n",
      "[-] Batch 100/172\t Loss: 0.002672\n",
      "\u001b[92m[-] Epoch 105/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.94(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 105/300\tVal loss: 0.63 \tacc: 90.59(%)\tfm: 67.84(%)\tfw: 90.94(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000320\n",
      "[-] Batch 100/172\t Loss: 0.003049\n",
      "\u001b[92m[-] Epoch 106/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.97(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 106/300\tVal loss: 0.73 \tacc: 90.24(%)\tfm: 68.63(%)\tfw: 90.75(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.008182\n",
      "[-] Batch 100/172\t Loss: 0.002521\n",
      "\u001b[92m[-] Epoch 107/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.94(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 107/300\tVal loss: 0.69 \tacc: 90.87(%)\tfm: 67.54(%)\tfw: 91.08(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000788\n",
      "[-] Batch 100/172\t Loss: 0.003453\n",
      "\u001b[92m[-] Epoch 108/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.89(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 108/300\tVal loss: 0.77 \tacc: 89.72(%)\tfm: 66.46(%)\tfw: 90.24(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0003486784401\n",
      "[-] Batch 0/172\t Loss: 0.000155\n",
      "[-] Batch 100/172\t Loss: 0.003713\n",
      "\u001b[92m[-] Epoch 109/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.95(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 109/300\tVal loss: 0.60 \tacc: 91.59(%)\tfm: 69.80(%)\tfw: 91.74(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.001451\n",
      "[-] Batch 100/172\t Loss: 0.002552\n",
      "\u001b[92m[-] Epoch 110/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.93(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 110/300\tVal loss: 0.69 \tacc: 90.12(%)\tfm: 62.94(%)\tfw: 90.28(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.006616\n",
      "[-] Batch 100/172\t Loss: 0.001689\n",
      "\u001b[92m[-] Epoch 111/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.99(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 111/300\tVal loss: 0.70 \tacc: 90.83(%)\tfm: 69.24(%)\tfw: 91.23(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.010856\n",
      "[-] Batch 100/172\t Loss: 0.001236\n",
      "\u001b[92m[-] Epoch 112/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.88(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 112/300\tVal loss: 0.65 \tacc: 91.55(%)\tfm: 70.47(%)\tfw: 91.67(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.000244\n",
      "[-] Batch 100/172\t Loss: 0.002302\n",
      "\u001b[92m[-] Epoch 113/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.95(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 113/300\tVal loss: 0.64 \tacc: 90.83(%)\tfm: 69.18(%)\tfw: 91.22(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.000160\n",
      "[-] Batch 100/172\t Loss: 0.003269\n",
      "\u001b[92m[-] Epoch 114/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.96(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 114/300\tVal loss: 0.73 \tacc: 90.39(%)\tfm: 66.19(%)\tfw: 90.68(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.002291\n",
      "[-] Batch 100/172\t Loss: 0.002101\n",
      "\u001b[92m[-] Epoch 115/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 115/300\tVal loss: 0.74 \tacc: 90.35(%)\tfm: 67.04(%)\tfw: 90.68(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.000420\n",
      "[-] Batch 100/172\t Loss: 0.001270\n",
      "\u001b[92m[-] Epoch 116/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.97(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 116/300\tVal loss: 0.71 \tacc: 90.59(%)\tfm: 68.97(%)\tfw: 91.10(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.000585\n",
      "[-] Batch 100/172\t Loss: 0.001756\n",
      "\u001b[92m[-] Epoch 117/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.95(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 117/300\tVal loss: 0.70 \tacc: 91.11(%)\tfm: 69.33(%)\tfw: 91.49(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.001598\n",
      "[-] Batch 100/172\t Loss: 0.002501\n",
      "\u001b[92m[-] Epoch 118/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.97(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 118/300\tVal loss: 0.75 \tacc: 90.20(%)\tfm: 68.74(%)\tfw: 90.76(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00031381059609000004\n",
      "[-] Batch 0/172\t Loss: 0.001148\n",
      "[-] Batch 100/172\t Loss: 0.002702\n",
      "\u001b[92m[-] Epoch 119/300\tTrain loss: 0.00 \tacc: 99.92(%)\tfm: 99.94(%)\tfw: 99.93(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 119/300\tVal loss: 0.81 \tacc: 89.68(%)\tfm: 70.05(%)\tfw: 90.42(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.000378\n",
      "[-] Batch 100/172\t Loss: 0.002897\n",
      "\u001b[92m[-] Epoch 120/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.96(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 120/300\tVal loss: 0.73 \tacc: 90.71(%)\tfm: 70.12(%)\tfw: 91.12(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.000369\n",
      "[-] Batch 100/172\t Loss: 0.003753\n",
      "\u001b[92m[-] Epoch 121/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.96(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 121/300\tVal loss: 0.63 \tacc: 91.39(%)\tfm: 69.73(%)\tfw: 91.69(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.001091\n",
      "[-] Batch 100/172\t Loss: 0.001628\n",
      "\u001b[92m[-] Epoch 122/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.99(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 122/300\tVal loss: 0.63 \tacc: 91.31(%)\tfm: 69.05(%)\tfw: 91.49(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.000112\n",
      "[-] Batch 100/172\t Loss: 0.001039\n",
      "\u001b[92m[-] Epoch 123/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.91(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 123/300\tVal loss: 0.75 \tacc: 90.43(%)\tfm: 65.56(%)\tfw: 90.82(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.000343\n",
      "[-] Batch 100/172\t Loss: 0.001061\n",
      "\u001b[92m[-] Epoch 124/300\tTrain loss: 0.00 \tacc: 99.94(%)\tfm: 99.92(%)\tfw: 99.94(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 124/300\tVal loss: 0.66 \tacc: 90.91(%)\tfm: 68.26(%)\tfw: 91.18(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.017097\n",
      "[-] Batch 100/172\t Loss: 0.002203\n",
      "\u001b[92m[-] Epoch 125/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.94(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 125/300\tVal loss: 0.71 \tacc: 90.83(%)\tfm: 67.27(%)\tfw: 91.16(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.001848\n",
      "[-] Batch 100/172\t Loss: 0.002399\n",
      "\u001b[92m[-] Epoch 126/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 126/300\tVal loss: 0.61 \tacc: 91.67(%)\tfm: 71.17(%)\tfw: 92.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.003669\n",
      "[-] Batch 100/172\t Loss: 0.000862\n",
      "\u001b[92m[-] Epoch 127/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.98(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 127/300\tVal loss: 0.71 \tacc: 90.95(%)\tfm: 70.28(%)\tfw: 91.44(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.001564\n",
      "[-] Batch 100/172\t Loss: 0.000702\n",
      "\u001b[92m[-] Epoch 128/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.86(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 128/300\tVal loss: 0.60 \tacc: 92.31(%)\tfm: 69.35(%)\tfw: 92.34(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00028242953648100003\n",
      "[-] Batch 0/172\t Loss: 0.000737\n",
      "[-] Batch 100/172\t Loss: 0.001520\n",
      "\u001b[92m[-] Epoch 129/300\tTrain loss: 0.00 \tacc: 99.95(%)\tfm: 99.86(%)\tfw: 99.95(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 129/300\tVal loss: 0.65 \tacc: 91.43(%)\tfm: 69.87(%)\tfw: 91.73(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.000100\n",
      "[-] Batch 100/172\t Loss: 0.002323\n",
      "\u001b[92m[-] Epoch 130/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 130/300\tVal loss: 0.65 \tacc: 91.39(%)\tfm: 68.66(%)\tfw: 91.66(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.000053\n",
      "[-] Batch 100/172\t Loss: 0.000981\n",
      "\u001b[92m[-] Epoch 131/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.96(%)\tfw: 99.97(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 131/300\tVal loss: 0.66 \tacc: 91.19(%)\tfm: 66.49(%)\tfw: 91.39(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.000148\n",
      "[-] Batch 100/172\t Loss: 0.002231\n",
      "\u001b[92m[-] Epoch 132/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.93(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 132/300\tVal loss: 0.82 \tacc: 90.12(%)\tfm: 71.15(%)\tfw: 90.85(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.001199\n",
      "[-] Batch 100/172\t Loss: 0.002100\n",
      "\u001b[92m[-] Epoch 133/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.97(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 133/300\tVal loss: 0.63 \tacc: 91.59(%)\tfm: 69.53(%)\tfw: 91.76(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.001580\n",
      "[-] Batch 100/172\t Loss: 0.001580\n",
      "\u001b[92m[-] Epoch 134/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 134/300\tVal loss: 0.67 \tacc: 91.91(%)\tfm: 71.18(%)\tfw: 92.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.000129\n",
      "[-] Batch 100/172\t Loss: 0.001356\n",
      "\u001b[92m[-] Epoch 135/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.99(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 135/300\tVal loss: 0.72 \tacc: 91.27(%)\tfm: 69.07(%)\tfw: 91.42(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.019051\n",
      "[-] Batch 100/172\t Loss: 0.003140\n",
      "\u001b[92m[-] Epoch 136/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.98(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 136/300\tVal loss: 0.62 \tacc: 91.91(%)\tfm: 69.24(%)\tfw: 92.03(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.000090\n",
      "[-] Batch 100/172\t Loss: 0.001950\n",
      "\u001b[92m[-] Epoch 137/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.97(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 137/300\tVal loss: 0.65 \tacc: 90.95(%)\tfm: 67.10(%)\tfw: 91.31(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.002092\n",
      "[-] Batch 100/172\t Loss: 0.001237\n",
      "\u001b[92m[-] Epoch 138/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.97(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 138/300\tVal loss: 0.71 \tacc: 90.83(%)\tfm: 67.21(%)\tfw: 91.19(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00025418658283290005\n",
      "[-] Batch 0/172\t Loss: 0.000055\n",
      "[-] Batch 100/172\t Loss: 0.000634\n",
      "\u001b[92m[-] Epoch 139/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 139/300\tVal loss: 0.69 \tacc: 91.11(%)\tfm: 67.48(%)\tfw: 91.39(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.001058\n",
      "[-] Batch 100/172\t Loss: 0.001290\n",
      "\u001b[92m[-] Epoch 140/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 140/300\tVal loss: 0.75 \tacc: 91.11(%)\tfm: 71.25(%)\tfw: 91.49(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000112\n",
      "[-] Batch 100/172\t Loss: 0.000599\n",
      "\u001b[92m[-] Epoch 141/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 100.00(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 141/300\tVal loss: 0.72 \tacc: 91.27(%)\tfm: 72.02(%)\tfw: 91.75(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7193598794044695->0.7202346963660667)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000074\n",
      "[-] Batch 100/172\t Loss: 0.000692\n",
      "\u001b[92m[-] Epoch 142/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 142/300\tVal loss: 0.79 \tacc: 90.43(%)\tfm: 70.47(%)\tfw: 91.05(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000148\n",
      "[-] Batch 100/172\t Loss: 0.000281\n",
      "\u001b[92m[-] Epoch 143/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 143/300\tVal loss: 0.74 \tacc: 91.07(%)\tfm: 70.42(%)\tfw: 91.50(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000082\n",
      "[-] Batch 100/172\t Loss: 0.000410\n",
      "\u001b[92m[-] Epoch 144/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 144/300\tVal loss: 0.87 \tacc: 90.24(%)\tfm: 70.85(%)\tfw: 90.84(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000054\n",
      "[-] Batch 100/172\t Loss: 0.001693\n",
      "\u001b[92m[-] Epoch 145/300\tTrain loss: 0.00 \tacc: 99.94(%)\tfm: 99.91(%)\tfw: 99.94(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 145/300\tVal loss: 0.78 \tacc: 90.28(%)\tfm: 68.28(%)\tfw: 90.91(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000239\n",
      "[-] Batch 100/172\t Loss: 0.002042\n",
      "\u001b[92m[-] Epoch 146/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 146/300\tVal loss: 0.70 \tacc: 91.35(%)\tfm: 70.76(%)\tfw: 91.62(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000705\n",
      "[-] Batch 100/172\t Loss: 0.001163\n",
      "\u001b[92m[-] Epoch 147/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 147/300\tVal loss: 0.68 \tacc: 91.27(%)\tfm: 69.77(%)\tfw: 91.62(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000017\n",
      "[-] Batch 100/172\t Loss: 0.000312\n",
      "\u001b[92m[-] Epoch 148/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 148/300\tVal loss: 0.72 \tacc: 91.11(%)\tfm: 69.97(%)\tfw: 91.47(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00022876792454961005\n",
      "[-] Batch 0/172\t Loss: 0.000032\n",
      "[-] Batch 100/172\t Loss: 0.001022\n",
      "\u001b[92m[-] Epoch 149/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.99(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 149/300\tVal loss: 0.77 \tacc: 90.71(%)\tfm: 69.43(%)\tfw: 91.12(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.002705\n",
      "[-] Batch 100/172\t Loss: 0.000578\n",
      "\u001b[92m[-] Epoch 150/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.97(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 150/300\tVal loss: 0.65 \tacc: 91.51(%)\tfm: 70.82(%)\tfw: 91.84(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.000384\n",
      "[-] Batch 100/172\t Loss: 0.001499\n",
      "\u001b[92m[-] Epoch 151/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.98(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 151/300\tVal loss: 0.68 \tacc: 90.99(%)\tfm: 71.22(%)\tfw: 91.39(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.006649\n",
      "[-] Batch 100/172\t Loss: 0.000998\n",
      "\u001b[92m[-] Epoch 152/300\tTrain loss: 0.00 \tacc: 99.90(%)\tfm: 99.81(%)\tfw: 99.90(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 152/300\tVal loss: 0.77 \tacc: 90.55(%)\tfm: 69.08(%)\tfw: 91.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.000114\n",
      "[-] Batch 100/172\t Loss: 0.000607\n",
      "\u001b[92m[-] Epoch 153/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 153/300\tVal loss: 0.70 \tacc: 91.55(%)\tfm: 67.73(%)\tfw: 91.69(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.000023\n",
      "[-] Batch 100/172\t Loss: 0.001126\n",
      "\u001b[92m[-] Epoch 154/300\tTrain loss: 0.00 \tacc: 99.96(%)\tfm: 99.93(%)\tfw: 99.96(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 154/300\tVal loss: 0.70 \tacc: 91.91(%)\tfm: 72.35(%)\tfw: 92.19(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7202346963660667->0.7235286427061226)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.002066\n",
      "[-] Batch 100/172\t Loss: 0.002411\n",
      "\u001b[92m[-] Epoch 155/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 155/300\tVal loss: 0.67 \tacc: 91.55(%)\tfm: 71.56(%)\tfw: 91.86(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.000072\n",
      "[-] Batch 100/172\t Loss: 0.000865\n",
      "\u001b[92m[-] Epoch 156/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 156/300\tVal loss: 0.72 \tacc: 91.27(%)\tfm: 71.19(%)\tfw: 91.68(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.000821\n",
      "[-] Batch 100/172\t Loss: 0.001748\n",
      "\u001b[92m[-] Epoch 157/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.97(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 157/300\tVal loss: 0.84 \tacc: 90.55(%)\tfm: 69.90(%)\tfw: 91.06(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.000041\n",
      "[-] Batch 100/172\t Loss: 0.000775\n",
      "\u001b[92m[-] Epoch 158/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 158/300\tVal loss: 0.76 \tacc: 91.07(%)\tfm: 69.07(%)\tfw: 91.29(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00020589113209464906\n",
      "[-] Batch 0/172\t Loss: 0.000260\n",
      "[-] Batch 100/172\t Loss: 0.000655\n",
      "\u001b[92m[-] Epoch 159/300\tTrain loss: 0.00 \tacc: 99.98(%)\tfm: 99.94(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 159/300\tVal loss: 0.84 \tacc: 90.16(%)\tfm: 70.47(%)\tfw: 90.78(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.002667\n",
      "[-] Batch 100/172\t Loss: 0.000718\n",
      "\u001b[92m[-] Epoch 160/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 160/300\tVal loss: 0.72 \tacc: 91.27(%)\tfm: 69.44(%)\tfw: 91.56(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000050\n",
      "[-] Batch 100/172\t Loss: 0.000507\n",
      "\u001b[92m[-] Epoch 161/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 161/300\tVal loss: 0.77 \tacc: 90.91(%)\tfm: 70.91(%)\tfw: 91.32(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000132\n",
      "[-] Batch 100/172\t Loss: 0.000587\n",
      "\u001b[92m[-] Epoch 162/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 162/300\tVal loss: 0.81 \tacc: 90.55(%)\tfm: 67.54(%)\tfw: 90.79(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000051\n",
      "[-] Batch 100/172\t Loss: 0.000599\n",
      "\u001b[92m[-] Epoch 163/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 163/300\tVal loss: 0.75 \tacc: 90.99(%)\tfm: 69.82(%)\tfw: 91.37(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000029\n",
      "[-] Batch 100/172\t Loss: 0.000773\n",
      "\u001b[92m[-] Epoch 164/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 164/300\tVal loss: 0.81 \tacc: 90.71(%)\tfm: 69.55(%)\tfw: 91.16(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000076\n",
      "[-] Batch 100/172\t Loss: 0.000808\n",
      "\u001b[92m[-] Epoch 165/300\tTrain loss: 0.00 \tacc: 99.97(%)\tfm: 99.99(%)\tfw: 99.98(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 165/300\tVal loss: 0.88 \tacc: 90.16(%)\tfm: 69.65(%)\tfw: 90.77(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000483\n",
      "[-] Batch 100/172\t Loss: 0.000208\n",
      "\u001b[92m[-] Epoch 166/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.97(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 166/300\tVal loss: 0.80 \tacc: 90.71(%)\tfm: 69.75(%)\tfw: 91.15(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000162\n",
      "[-] Batch 100/172\t Loss: 0.001152\n",
      "\u001b[92m[-] Epoch 167/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 167/300\tVal loss: 0.72 \tacc: 91.03(%)\tfm: 70.02(%)\tfw: 91.45(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000122\n",
      "[-] Batch 100/172\t Loss: 0.000712\n",
      "\u001b[92m[-] Epoch 168/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 168/300\tVal loss: 0.77 \tacc: 90.87(%)\tfm: 70.93(%)\tfw: 91.29(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00018530201888518417\n",
      "[-] Batch 0/172\t Loss: 0.000038\n",
      "[-] Batch 100/172\t Loss: 0.000607\n",
      "\u001b[92m[-] Epoch 169/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 169/300\tVal loss: 0.78 \tacc: 91.11(%)\tfm: 70.93(%)\tfw: 91.48(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000025\n",
      "[-] Batch 100/172\t Loss: 0.000289\n",
      "\u001b[92m[-] Epoch 170/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 170/300\tVal loss: 0.79 \tacc: 90.91(%)\tfm: 69.89(%)\tfw: 91.25(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000020\n",
      "[-] Batch 100/172\t Loss: 0.000276\n",
      "\u001b[92m[-] Epoch 171/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 171/300\tVal loss: 0.79 \tacc: 90.99(%)\tfm: 71.47(%)\tfw: 91.41(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000024\n",
      "[-] Batch 100/172\t Loss: 0.000340\n",
      "\u001b[92m[-] Epoch 172/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 172/300\tVal loss: 0.77 \tacc: 91.47(%)\tfm: 72.68(%)\tfw: 91.76(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7235286427061226->0.7267781421585741)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000206\n",
      "[-] Batch 100/172\t Loss: 0.000480\n",
      "\u001b[92m[-] Epoch 173/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 173/300\tVal loss: 0.79 \tacc: 91.15(%)\tfm: 71.60(%)\tfw: 91.50(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000176\n",
      "[-] Batch 100/172\t Loss: 0.000534\n",
      "\u001b[92m[-] Epoch 174/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.99(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 174/300\tVal loss: 0.85 \tacc: 90.47(%)\tfm: 69.22(%)\tfw: 91.06(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000016\n",
      "[-] Batch 100/172\t Loss: 0.000455\n",
      "\u001b[92m[-] Epoch 175/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 175/300\tVal loss: 0.84 \tacc: 90.87(%)\tfm: 71.86(%)\tfw: 91.34(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000093\n",
      "[-] Batch 100/172\t Loss: 0.001012\n",
      "\u001b[92m[-] Epoch 176/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 176/300\tVal loss: 0.73 \tacc: 91.47(%)\tfm: 73.99(%)\tfw: 91.91(%)\u001b[0m\n",
      "\u001b[94m[*] Saving checkpoint... (0.7267781421585741->0.7398734438229001)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000090\n",
      "[-] Batch 100/172\t Loss: 0.001056\n",
      "\u001b[92m[-] Epoch 177/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 177/300\tVal loss: 0.61 \tacc: 92.71(%)\tfm: 70.29(%)\tfw: 92.64(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000062\n",
      "[-] Batch 100/172\t Loss: 0.000282\n",
      "\u001b[92m[-] Epoch 178/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 178/300\tVal loss: 0.75 \tacc: 91.63(%)\tfm: 73.02(%)\tfw: 91.92(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00016677181699666576\n",
      "[-] Batch 0/172\t Loss: 0.000115\n",
      "[-] Batch 100/172\t Loss: 0.000622\n",
      "\u001b[92m[-] Epoch 179/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 179/300\tVal loss: 0.69 \tacc: 91.55(%)\tfm: 69.14(%)\tfw: 91.75(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000163\n",
      "[-] Batch 100/172\t Loss: 0.001098\n",
      "\u001b[92m[-] Epoch 180/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 180/300\tVal loss: 0.81 \tacc: 90.95(%)\tfm: 70.13(%)\tfw: 91.27(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000226\n",
      "[-] Batch 100/172\t Loss: 0.000363\n",
      "\u001b[92m[-] Epoch 181/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 181/300\tVal loss: 0.72 \tacc: 91.63(%)\tfm: 70.58(%)\tfw: 91.76(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000072\n",
      "[-] Batch 100/172\t Loss: 0.000193\n",
      "\u001b[92m[-] Epoch 182/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 182/300\tVal loss: 0.75 \tacc: 91.39(%)\tfm: 71.15(%)\tfw: 91.63(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000026\n",
      "[-] Batch 100/172\t Loss: 0.000271\n",
      "\u001b[92m[-] Epoch 183/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 183/300\tVal loss: 0.79 \tacc: 91.43(%)\tfm: 70.14(%)\tfw: 91.64(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000132\n",
      "[-] Batch 100/172\t Loss: 0.000124\n",
      "\u001b[92m[-] Epoch 184/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 184/300\tVal loss: 0.79 \tacc: 91.47(%)\tfm: 70.93(%)\tfw: 91.69(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000058\n",
      "[-] Batch 100/172\t Loss: 0.000090\n",
      "\u001b[92m[-] Epoch 185/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 185/300\tVal loss: 0.78 \tacc: 91.19(%)\tfm: 71.62(%)\tfw: 91.57(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000037\n",
      "[-] Batch 100/172\t Loss: 0.001008\n",
      "\u001b[92m[-] Epoch 186/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 186/300\tVal loss: 0.74 \tacc: 91.23(%)\tfm: 68.41(%)\tfw: 91.42(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000036\n",
      "[-] Batch 100/172\t Loss: 0.000391\n",
      "\u001b[92m[-] Epoch 187/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 187/300\tVal loss: 0.76 \tacc: 91.23(%)\tfm: 71.20(%)\tfw: 91.56(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.001154\n",
      "[-] Batch 100/172\t Loss: 0.001312\n",
      "\u001b[92m[-] Epoch 188/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 188/300\tVal loss: 0.72 \tacc: 90.59(%)\tfm: 69.59(%)\tfw: 91.06(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001500946352969992\n",
      "[-] Batch 0/172\t Loss: 0.000071\n",
      "[-] Batch 100/172\t Loss: 0.000498\n",
      "\u001b[92m[-] Epoch 189/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 189/300\tVal loss: 0.83 \tacc: 90.39(%)\tfm: 70.80(%)\tfw: 90.99(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000044\n",
      "[-] Batch 100/172\t Loss: 0.000388\n",
      "\u001b[92m[-] Epoch 190/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 190/300\tVal loss: 0.80 \tacc: 90.79(%)\tfm: 69.82(%)\tfw: 91.25(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000016\n",
      "[-] Batch 100/172\t Loss: 0.000133\n",
      "\u001b[92m[-] Epoch 191/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 191/300\tVal loss: 0.72 \tacc: 91.51(%)\tfm: 71.32(%)\tfw: 91.86(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000012\n",
      "[-] Batch 100/172\t Loss: 0.000115\n",
      "\u001b[92m[-] Epoch 192/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 192/300\tVal loss: 0.71 \tacc: 91.39(%)\tfm: 70.84(%)\tfw: 91.71(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000022\n",
      "[-] Batch 100/172\t Loss: 0.000098\n",
      "\u001b[92m[-] Epoch 193/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 193/300\tVal loss: 0.77 \tacc: 91.19(%)\tfm: 70.99(%)\tfw: 91.54(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000041\n",
      "[-] Batch 100/172\t Loss: 0.000249\n",
      "\u001b[92m[-] Epoch 194/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 194/300\tVal loss: 0.75 \tacc: 91.35(%)\tfm: 69.97(%)\tfw: 91.63(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000038\n",
      "[-] Batch 100/172\t Loss: 0.000858\n",
      "\u001b[92m[-] Epoch 195/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 195/300\tVal loss: 0.75 \tacc: 91.35(%)\tfm: 69.47(%)\tfw: 91.53(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000018\n",
      "[-] Batch 100/172\t Loss: 0.000272\n",
      "\u001b[92m[-] Epoch 196/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 196/300\tVal loss: 0.71 \tacc: 91.71(%)\tfm: 69.83(%)\tfw: 91.87(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000014\n",
      "[-] Batch 100/172\t Loss: 0.000140\n",
      "\u001b[92m[-] Epoch 197/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 197/300\tVal loss: 0.75 \tacc: 91.23(%)\tfm: 68.86(%)\tfw: 91.49(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000056\n",
      "[-] Batch 100/172\t Loss: 0.000306\n",
      "\u001b[92m[-] Epoch 198/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 198/300\tVal loss: 0.78 \tacc: 91.31(%)\tfm: 70.69(%)\tfw: 91.54(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.0001350851717672993\n",
      "[-] Batch 0/172\t Loss: 0.000006\n",
      "[-] Batch 100/172\t Loss: 0.001134\n",
      "\u001b[92m[-] Epoch 199/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.99(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 199/300\tVal loss: 0.74 \tacc: 91.55(%)\tfm: 69.84(%)\tfw: 91.75(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000021\n",
      "[-] Batch 100/172\t Loss: 0.000629\n",
      "\u001b[92m[-] Epoch 200/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 200/300\tVal loss: 0.82 \tacc: 90.55(%)\tfm: 70.67(%)\tfw: 91.07(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000008\n",
      "[-] Batch 100/172\t Loss: 0.000595\n",
      "\u001b[92m[-] Epoch 201/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 201/300\tVal loss: 0.71 \tacc: 91.63(%)\tfm: 69.10(%)\tfw: 91.79(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000029\n",
      "[-] Batch 100/172\t Loss: 0.000634\n",
      "\u001b[92m[-] Epoch 202/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 202/300\tVal loss: 0.67 \tacc: 91.91(%)\tfm: 69.12(%)\tfw: 91.99(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000013\n",
      "[-] Batch 100/172\t Loss: 0.000280\n",
      "\u001b[92m[-] Epoch 203/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 203/300\tVal loss: 0.70 \tacc: 91.83(%)\tfm: 70.72(%)\tfw: 91.99(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000039\n",
      "[-] Batch 100/172\t Loss: 0.000092\n",
      "\u001b[92m[-] Epoch 204/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 204/300\tVal loss: 0.69 \tacc: 91.87(%)\tfm: 70.24(%)\tfw: 92.05(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000033\n",
      "[-] Batch 100/172\t Loss: 0.000097\n",
      "\u001b[92m[-] Epoch 205/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 205/300\tVal loss: 0.81 \tacc: 90.71(%)\tfm: 68.87(%)\tfw: 91.06(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000264\n",
      "[-] Batch 100/172\t Loss: 0.000263\n",
      "\u001b[92m[-] Epoch 206/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 206/300\tVal loss: 0.76 \tacc: 91.27(%)\tfm: 69.00(%)\tfw: 91.54(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000020\n",
      "[-] Batch 100/172\t Loss: 0.000217\n",
      "\u001b[92m[-] Epoch 207/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 207/300\tVal loss: 0.73 \tacc: 91.51(%)\tfm: 70.45(%)\tfw: 91.79(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000131\n",
      "\u001b[92m[-] Epoch 208/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 208/300\tVal loss: 0.77 \tacc: 91.47(%)\tfm: 70.23(%)\tfw: 91.71(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00012157665459056936\n",
      "[-] Batch 0/172\t Loss: 0.000014\n",
      "[-] Batch 100/172\t Loss: 0.000059\n",
      "\u001b[92m[-] Epoch 209/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 209/300\tVal loss: 0.77 \tacc: 91.59(%)\tfm: 70.48(%)\tfw: 91.78(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000006\n",
      "[-] Batch 100/172\t Loss: 0.000089\n",
      "\u001b[92m[-] Epoch 210/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 210/300\tVal loss: 0.70 \tacc: 91.99(%)\tfm: 70.58(%)\tfw: 92.14(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000026\n",
      "[-] Batch 100/172\t Loss: 0.000073\n",
      "\u001b[92m[-] Epoch 211/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 211/300\tVal loss: 0.73 \tacc: 91.79(%)\tfm: 69.01(%)\tfw: 91.83(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000018\n",
      "[-] Batch 100/172\t Loss: 0.000029\n",
      "\u001b[92m[-] Epoch 212/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 212/300\tVal loss: 0.72 \tacc: 92.07(%)\tfm: 70.73(%)\tfw: 92.21(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.002891\n",
      "[-] Batch 100/172\t Loss: 0.000524\n",
      "\u001b[92m[-] Epoch 213/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 213/300\tVal loss: 0.73 \tacc: 92.03(%)\tfm: 70.15(%)\tfw: 92.02(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000029\n",
      "[-] Batch 100/172\t Loss: 0.000533\n",
      "\u001b[92m[-] Epoch 214/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 214/300\tVal loss: 0.77 \tacc: 91.63(%)\tfm: 68.50(%)\tfw: 91.68(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000044\n",
      "[-] Batch 100/172\t Loss: 0.001105\n",
      "\u001b[92m[-] Epoch 215/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 215/300\tVal loss: 0.65 \tacc: 92.39(%)\tfm: 69.65(%)\tfw: 92.38(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000038\n",
      "[-] Batch 100/172\t Loss: 0.000348\n",
      "\u001b[92m[-] Epoch 216/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 216/300\tVal loss: 0.75 \tacc: 91.59(%)\tfm: 68.27(%)\tfw: 91.64(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000122\n",
      "[-] Batch 100/172\t Loss: 0.000083\n",
      "\u001b[92m[-] Epoch 217/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.99(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 217/300\tVal loss: 0.78 \tacc: 91.31(%)\tfm: 70.41(%)\tfw: 91.63(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000547\n",
      "[-] Batch 100/172\t Loss: 0.000071\n",
      "\u001b[92m[-] Epoch 218/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 218/300\tVal loss: 0.76 \tacc: 91.59(%)\tfm: 70.55(%)\tfw: 91.88(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  0.00010941898913151243\n",
      "[-] Batch 0/172\t Loss: 0.000053\n",
      "[-] Batch 100/172\t Loss: 0.000651\n",
      "\u001b[92m[-] Epoch 219/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 219/300\tVal loss: 0.80 \tacc: 91.19(%)\tfm: 70.25(%)\tfw: 91.56(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000108\n",
      "\u001b[92m[-] Epoch 220/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 99.99(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 220/300\tVal loss: 0.72 \tacc: 91.75(%)\tfm: 69.19(%)\tfw: 91.98(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000035\n",
      "[-] Batch 100/172\t Loss: 0.000407\n",
      "\u001b[92m[-] Epoch 221/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 221/300\tVal loss: 0.76 \tacc: 91.47(%)\tfm: 68.73(%)\tfw: 91.61(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000006\n",
      "[-] Batch 100/172\t Loss: 0.000071\n",
      "\u001b[92m[-] Epoch 222/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 222/300\tVal loss: 0.76 \tacc: 91.71(%)\tfm: 69.72(%)\tfw: 91.88(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000043\n",
      "[-] Batch 100/172\t Loss: 0.000027\n",
      "\u001b[92m[-] Epoch 223/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 223/300\tVal loss: 0.76 \tacc: 91.83(%)\tfm: 70.55(%)\tfw: 91.93(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000011\n",
      "[-] Batch 100/172\t Loss: 0.000199\n",
      "\u001b[92m[-] Epoch 224/300\tTrain loss: 0.00 \tacc: 99.99(%)\tfm: 99.98(%)\tfw: 99.99(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 224/300\tVal loss: 0.85 \tacc: 90.31(%)\tfm: 70.65(%)\tfw: 90.97(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000009\n",
      "[-] Batch 100/172\t Loss: 0.000259\n",
      "\u001b[92m[-] Epoch 225/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 225/300\tVal loss: 0.75 \tacc: 91.43(%)\tfm: 70.14(%)\tfw: 91.67(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000030\n",
      "[-] Batch 100/172\t Loss: 0.000148\n",
      "\u001b[92m[-] Epoch 226/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 226/300\tVal loss: 0.80 \tacc: 90.83(%)\tfm: 70.36(%)\tfw: 91.28(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000996\n",
      "[-] Batch 100/172\t Loss: 0.000124\n",
      "\u001b[92m[-] Epoch 227/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 227/300\tVal loss: 0.85 \tacc: 90.87(%)\tfm: 71.40(%)\tfw: 91.38(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000005\n",
      "[-] Batch 100/172\t Loss: 0.000039\n",
      "\u001b[92m[-] Epoch 228/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 228/300\tVal loss: 0.86 \tacc: 90.75(%)\tfm: 70.86(%)\tfw: 91.28(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  9.847709021836118e-05\n",
      "[-] Batch 0/172\t Loss: 0.000008\n",
      "[-] Batch 100/172\t Loss: 0.000301\n",
      "\u001b[92m[-] Epoch 229/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 229/300\tVal loss: 0.79 \tacc: 91.35(%)\tfm: 70.62(%)\tfw: 91.69(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000030\n",
      "[-] Batch 100/172\t Loss: 0.000142\n",
      "\u001b[92m[-] Epoch 230/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 230/300\tVal loss: 0.76 \tacc: 91.39(%)\tfm: 68.92(%)\tfw: 91.63(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000012\n",
      "[-] Batch 100/172\t Loss: 0.000082\n",
      "\u001b[92m[-] Epoch 231/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 231/300\tVal loss: 0.78 \tacc: 91.07(%)\tfm: 68.76(%)\tfw: 91.36(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000016\n",
      "[-] Batch 100/172\t Loss: 0.000030\n",
      "\u001b[92m[-] Epoch 232/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 232/300\tVal loss: 0.80 \tacc: 91.07(%)\tfm: 69.49(%)\tfw: 91.38(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000008\n",
      "[-] Batch 100/172\t Loss: 0.000088\n",
      "\u001b[92m[-] Epoch 233/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 233/300\tVal loss: 0.84 \tacc: 91.03(%)\tfm: 69.84(%)\tfw: 91.41(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000010\n",
      "[-] Batch 100/172\t Loss: 0.000121\n",
      "\u001b[92m[-] Epoch 234/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 234/300\tVal loss: 0.80 \tacc: 91.27(%)\tfm: 69.41(%)\tfw: 91.57(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000005\n",
      "[-] Batch 100/172\t Loss: 0.000027\n",
      "\u001b[92m[-] Epoch 235/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 235/300\tVal loss: 0.81 \tacc: 91.07(%)\tfm: 68.52(%)\tfw: 91.35(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000011\n",
      "[-] Batch 100/172\t Loss: 0.000743\n",
      "\u001b[92m[-] Epoch 236/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 236/300\tVal loss: 0.72 \tacc: 92.03(%)\tfm: 69.66(%)\tfw: 92.21(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000130\n",
      "\u001b[92m[-] Epoch 237/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 237/300\tVal loss: 0.74 \tacc: 91.63(%)\tfm: 70.30(%)\tfw: 91.79(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000015\n",
      "[-] Batch 100/172\t Loss: 0.000232\n",
      "\u001b[92m[-] Epoch 238/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 238/300\tVal loss: 0.70 \tacc: 92.27(%)\tfm: 70.15(%)\tfw: 92.34(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  8.862938119652506e-05\n",
      "[-] Batch 0/172\t Loss: 0.000198\n",
      "[-] Batch 100/172\t Loss: 0.000113\n",
      "\u001b[92m[-] Epoch 239/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 239/300\tVal loss: 0.70 \tacc: 92.39(%)\tfm: 70.18(%)\tfw: 92.41(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000387\n",
      "[-] Batch 100/172\t Loss: 0.000274\n",
      "\u001b[92m[-] Epoch 240/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 240/300\tVal loss: 0.76 \tacc: 91.15(%)\tfm: 69.63(%)\tfw: 91.45(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000062\n",
      "\u001b[92m[-] Epoch 241/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 241/300\tVal loss: 0.69 \tacc: 92.47(%)\tfm: 69.66(%)\tfw: 92.46(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000305\n",
      "\u001b[92m[-] Epoch 242/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 242/300\tVal loss: 0.68 \tacc: 91.87(%)\tfm: 70.04(%)\tfw: 92.04(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000010\n",
      "[-] Batch 100/172\t Loss: 0.000073\n",
      "\u001b[92m[-] Epoch 243/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 243/300\tVal loss: 0.74 \tacc: 91.51(%)\tfm: 70.34(%)\tfw: 91.73(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000013\n",
      "[-] Batch 100/172\t Loss: 0.000080\n",
      "\u001b[92m[-] Epoch 244/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 244/300\tVal loss: 0.78 \tacc: 91.51(%)\tfm: 70.57(%)\tfw: 91.72(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000037\n",
      "\u001b[92m[-] Epoch 245/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 245/300\tVal loss: 0.75 \tacc: 91.75(%)\tfm: 70.18(%)\tfw: 91.93(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000023\n",
      "\u001b[92m[-] Epoch 246/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 246/300\tVal loss: 0.78 \tacc: 91.51(%)\tfm: 70.31(%)\tfw: 91.76(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000022\n",
      "\u001b[92m[-] Epoch 247/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 247/300\tVal loss: 0.80 \tacc: 91.47(%)\tfm: 70.09(%)\tfw: 91.62(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000013\n",
      "[-] Batch 100/172\t Loss: 0.000041\n",
      "\u001b[92m[-] Epoch 248/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 248/300\tVal loss: 0.82 \tacc: 91.51(%)\tfm: 70.55(%)\tfw: 91.78(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.976644307687256e-05\n",
      "[-] Batch 0/172\t Loss: 0.000017\n",
      "[-] Batch 100/172\t Loss: 0.000346\n",
      "\u001b[92m[-] Epoch 249/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 249/300\tVal loss: 0.80 \tacc: 91.59(%)\tfm: 70.13(%)\tfw: 91.81(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000005\n",
      "[-] Batch 100/172\t Loss: 0.000045\n",
      "\u001b[92m[-] Epoch 250/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 250/300\tVal loss: 0.86 \tacc: 91.15(%)\tfm: 70.66(%)\tfw: 91.54(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000012\n",
      "[-] Batch 100/172\t Loss: 0.000244\n",
      "\u001b[92m[-] Epoch 251/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 251/300\tVal loss: 0.86 \tacc: 90.71(%)\tfm: 69.44(%)\tfw: 91.14(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.001378\n",
      "[-] Batch 100/172\t Loss: 0.000119\n",
      "\u001b[92m[-] Epoch 252/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 252/300\tVal loss: 0.82 \tacc: 91.27(%)\tfm: 70.37(%)\tfw: 91.53(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000029\n",
      "\u001b[92m[-] Epoch 253/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 253/300\tVal loss: 0.83 \tacc: 91.23(%)\tfm: 70.76(%)\tfw: 91.54(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000122\n",
      "\u001b[92m[-] Epoch 254/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 254/300\tVal loss: 0.78 \tacc: 91.47(%)\tfm: 70.60(%)\tfw: 91.74(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000005\n",
      "[-] Batch 100/172\t Loss: 0.000020\n",
      "\u001b[92m[-] Epoch 255/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 255/300\tVal loss: 0.81 \tacc: 91.39(%)\tfm: 70.59(%)\tfw: 91.68(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000021\n",
      "\u001b[92m[-] Epoch 256/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 256/300\tVal loss: 0.82 \tacc: 91.31(%)\tfm: 70.72(%)\tfw: 91.62(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000006\n",
      "[-] Batch 100/172\t Loss: 0.000017\n",
      "\u001b[92m[-] Epoch 257/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 257/300\tVal loss: 0.78 \tacc: 91.63(%)\tfm: 70.78(%)\tfw: 91.85(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000005\n",
      "[-] Batch 100/172\t Loss: 0.000340\n",
      "\u001b[92m[-] Epoch 258/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 258/300\tVal loss: 0.78 \tacc: 91.47(%)\tfm: 70.89(%)\tfw: 91.78(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  7.17897987691853e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000094\n",
      "\u001b[92m[-] Epoch 259/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 259/300\tVal loss: 0.82 \tacc: 91.23(%)\tfm: 69.56(%)\tfw: 91.47(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000009\n",
      "[-] Batch 100/172\t Loss: 0.000272\n",
      "\u001b[92m[-] Epoch 260/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 260/300\tVal loss: 0.80 \tacc: 91.39(%)\tfm: 69.89(%)\tfw: 91.67(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000082\n",
      "\u001b[92m[-] Epoch 261/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 261/300\tVal loss: 0.89 \tacc: 90.75(%)\tfm: 70.46(%)\tfw: 91.23(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000018\n",
      "[-] Batch 100/172\t Loss: 0.000055\n",
      "\u001b[92m[-] Epoch 262/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 262/300\tVal loss: 0.83 \tacc: 91.27(%)\tfm: 70.57(%)\tfw: 91.58(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000022\n",
      "\u001b[92m[-] Epoch 263/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 263/300\tVal loss: 0.81 \tacc: 91.55(%)\tfm: 71.30(%)\tfw: 91.87(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000011\n",
      "\u001b[92m[-] Epoch 264/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 264/300\tVal loss: 0.80 \tacc: 91.63(%)\tfm: 70.30(%)\tfw: 91.86(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000041\n",
      "[-] Batch 100/172\t Loss: 0.000020\n",
      "\u001b[92m[-] Epoch 265/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 265/300\tVal loss: 0.76 \tacc: 91.83(%)\tfm: 70.36(%)\tfw: 92.02(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000012\n",
      "[-] Batch 100/172\t Loss: 0.000055\n",
      "\u001b[92m[-] Epoch 266/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 266/300\tVal loss: 0.76 \tacc: 92.07(%)\tfm: 71.11(%)\tfw: 92.23(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000208\n",
      "\u001b[92m[-] Epoch 267/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 267/300\tVal loss: 0.77 \tacc: 92.19(%)\tfm: 71.45(%)\tfw: 92.30(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000034\n",
      "\u001b[92m[-] Epoch 268/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 268/300\tVal loss: 0.80 \tacc: 91.75(%)\tfm: 71.41(%)\tfw: 92.04(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  6.461081889226677e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000030\n",
      "\u001b[92m[-] Epoch 269/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 269/300\tVal loss: 0.90 \tacc: 90.91(%)\tfm: 70.79(%)\tfw: 91.34(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000087\n",
      "\u001b[92m[-] Epoch 270/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 270/300\tVal loss: 0.95 \tacc: 90.63(%)\tfm: 71.50(%)\tfw: 91.24(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000135\n",
      "[-] Batch 100/172\t Loss: 0.000049\n",
      "\u001b[92m[-] Epoch 271/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 271/300\tVal loss: 0.75 \tacc: 92.07(%)\tfm: 71.31(%)\tfw: 92.19(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000205\n",
      "\u001b[92m[-] Epoch 272/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 272/300\tVal loss: 0.89 \tacc: 90.99(%)\tfm: 70.22(%)\tfw: 91.41(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000014\n",
      "[-] Batch 100/172\t Loss: 0.000026\n",
      "\u001b[92m[-] Epoch 273/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 273/300\tVal loss: 0.88 \tacc: 91.19(%)\tfm: 70.08(%)\tfw: 91.56(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000043\n",
      "[-] Batch 100/172\t Loss: 0.000113\n",
      "\u001b[92m[-] Epoch 274/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 274/300\tVal loss: 0.84 \tacc: 91.51(%)\tfm: 68.95(%)\tfw: 91.68(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000092\n",
      "\u001b[92m[-] Epoch 275/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 275/300\tVal loss: 0.74 \tacc: 92.39(%)\tfm: 70.63(%)\tfw: 92.45(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000046\n",
      "\u001b[92m[-] Epoch 276/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 276/300\tVal loss: 0.85 \tacc: 91.39(%)\tfm: 70.78(%)\tfw: 91.72(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000018\n",
      "\u001b[92m[-] Epoch 277/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 277/300\tVal loss: 0.72 \tacc: 92.83(%)\tfm: 71.33(%)\tfw: 92.81(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000057\n",
      "\u001b[92m[-] Epoch 278/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 278/300\tVal loss: 0.80 \tacc: 91.83(%)\tfm: 70.86(%)\tfw: 92.01(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.81497370030401e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000219\n",
      "\u001b[92m[-] Epoch 279/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 279/300\tVal loss: 0.66 \tacc: 93.18(%)\tfm: 71.14(%)\tfw: 93.08(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000066\n",
      "[-] Batch 100/172\t Loss: 0.000184\n",
      "\u001b[92m[-] Epoch 280/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 280/300\tVal loss: 0.76 \tacc: 92.07(%)\tfm: 71.10(%)\tfw: 92.13(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000138\n",
      "\u001b[92m[-] Epoch 281/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 281/300\tVal loss: 0.83 \tacc: 91.11(%)\tfm: 71.19(%)\tfw: 91.51(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000015\n",
      "[-] Batch 100/172\t Loss: 0.000024\n",
      "\u001b[92m[-] Epoch 282/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 282/300\tVal loss: 0.83 \tacc: 91.19(%)\tfm: 70.89(%)\tfw: 91.55(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000014\n",
      "\u001b[92m[-] Epoch 283/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 283/300\tVal loss: 0.81 \tacc: 91.51(%)\tfm: 71.56(%)\tfw: 91.82(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000004\n",
      "[-] Batch 100/172\t Loss: 0.000023\n",
      "\u001b[92m[-] Epoch 284/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 284/300\tVal loss: 0.80 \tacc: 91.55(%)\tfm: 71.14(%)\tfw: 91.80(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000011\n",
      "\u001b[92m[-] Epoch 285/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 285/300\tVal loss: 0.80 \tacc: 91.63(%)\tfm: 71.65(%)\tfw: 91.90(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000005\n",
      "[-] Batch 100/172\t Loss: 0.000028\n",
      "\u001b[92m[-] Epoch 286/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 286/300\tVal loss: 0.89 \tacc: 90.87(%)\tfm: 70.57(%)\tfw: 91.30(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000065\n",
      "\u001b[92m[-] Epoch 287/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 287/300\tVal loss: 0.82 \tacc: 91.63(%)\tfm: 70.23(%)\tfw: 91.83(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000031\n",
      "\u001b[92m[-] Epoch 288/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 288/300\tVal loss: 0.82 \tacc: 91.59(%)\tfm: 70.21(%)\tfw: 91.80(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  5.233476330273609e-05\n",
      "[-] Batch 0/172\t Loss: 0.000011\n",
      "[-] Batch 100/172\t Loss: 0.000258\n",
      "\u001b[92m[-] Epoch 289/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 289/300\tVal loss: 0.81 \tacc: 91.55(%)\tfm: 70.94(%)\tfw: 91.83(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000006\n",
      "[-] Batch 100/172\t Loss: 0.000182\n",
      "\u001b[92m[-] Epoch 290/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 290/300\tVal loss: 0.81 \tacc: 91.99(%)\tfm: 70.59(%)\tfw: 92.09(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000062\n",
      "\u001b[92m[-] Epoch 291/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 291/300\tVal loss: 0.79 \tacc: 91.79(%)\tfm: 70.77(%)\tfw: 92.00(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000007\n",
      "[-] Batch 100/172\t Loss: 0.000014\n",
      "\u001b[92m[-] Epoch 292/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 292/300\tVal loss: 0.83 \tacc: 91.59(%)\tfm: 71.41(%)\tfw: 91.88(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000005\n",
      "[-] Batch 100/172\t Loss: 0.000041\n",
      "\u001b[92m[-] Epoch 293/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 293/300\tVal loss: 0.81 \tacc: 91.67(%)\tfm: 71.12(%)\tfw: 91.94(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000130\n",
      "\u001b[92m[-] Epoch 294/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 294/300\tVal loss: 0.82 \tacc: 91.79(%)\tfm: 71.72(%)\tfw: 92.03(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000003\n",
      "[-] Batch 100/172\t Loss: 0.000010\n",
      "\u001b[92m[-] Epoch 295/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 295/300\tVal loss: 0.83 \tacc: 91.59(%)\tfm: 71.29(%)\tfw: 91.86(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000011\n",
      "[-] Batch 100/172\t Loss: 0.000027\n",
      "\u001b[92m[-] Epoch 296/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 296/300\tVal loss: 0.82 \tacc: 91.83(%)\tfm: 71.33(%)\tfw: 92.05(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000009\n",
      "[-] Batch 100/172\t Loss: 0.000042\n",
      "\u001b[92m[-] Epoch 297/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 297/300\tVal loss: 0.83 \tacc: 91.59(%)\tfm: 71.90(%)\tfw: 91.93(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000017\n",
      "[-] Batch 100/172\t Loss: 0.000129\n",
      "\u001b[92m[-] Epoch 298/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 298/300\tVal loss: 0.82 \tacc: 91.67(%)\tfm: 70.87(%)\tfw: 91.92(%)\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[-] Learning rate:  4.7101286972462485e-05\n",
      "[-] Batch 0/172\t Loss: 0.000009\n",
      "[-] Batch 100/172\t Loss: 0.000050\n",
      "\u001b[92m[-] Epoch 299/300\tTrain loss: 0.00 \tacc: 100.00(%)\tfm: 100.00(%)\tfw: 100.00(%)\t\u001b[0m\n",
      "\u001b[92m[-] Epoch 299/300\tVal loss: 0.81 \tacc: 91.71(%)\tfm: 70.46(%)\tfw: 91.89(%)\u001b[0m\n",
      "\u001b[92mFinished HAR training loop (h:m:s): 5:51:37\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from DeepConvLSTM_py3 import model_train\n",
    "\n",
    "# Define train config options\n",
    "config_train = {'batch_size': 256,\n",
    "                'optimizer': 'Adam',\n",
    "                'lr': 1e-3,\n",
    "                'lr_step': 10,\n",
    "                'lr_decay': 0.9,\n",
    "                'init_weights': 'orthogonal',\n",
    "                'epochs': 300,\n",
    "                'print_freq': 100\n",
    "               }\n",
    "\n",
    "model_train(deepconv, dataset, dataset_val, config_train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data to evaluate the trained model, and setup the test configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mCreating opportunity test HAR dataset of size 118726 ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset_test = SensorDataset(**config_dataset, prefix=\"test\")\n",
    "test_config = {'batch_size': 256,\n",
    "              'train_mode': False,\n",
    "              'dataset': target_dataset,\n",
    "              'num_batches_eval': 212}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model_eval and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mRunning HAR evaluation loop ...\u001b[0m\n",
      "[-] Loading checkpoint ...\n",
      "\u001b[92m[-] Test loss: 0.93\tacc: 88.55(%)\tfm: 62.12(%)\tfw: 88.93(%)\u001b[0m\n",
      "\u001b[92m[Finished HAR evaluation loop (h:m:s): 0:01:18\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from DeepConvLSTM_py3 import model_eval\n",
    "model_eval(deepconv, dataset_test, test_config, return_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use [thop](https://pypi.org/project/thop/) to count the number of floating point operations performed during a forward pass of one batch of synthetic data. We divide by the batch size to get the number of operations per window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_lstm() for <class 'torch.nn.modules.rnn.LSTM'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'DeepConvLSTM_py3.DeepConvLSTM'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "Number of floating point operations per forward pass of one sliding window segment: 115,671,040.0\n",
      "Number of parameters: 3,965,778.0\n"
     ]
    }
   ],
   "source": [
    "# Get number of flops\n",
    "from thop import profile\n",
    "import torch\n",
    "deepconv = deepconv.train()\n",
    "\n",
    "x = torch.ones([config_train['batch_size'], config_dataset['window'], n_channels]).cuda()\n",
    "macs, params = profile(deepconv, inputs=(x,), verbose=True)\n",
    "flops = macs / config_train['batch_size']\n",
    "print(f'Number of floating point operations per forward pass of one sliding window segment: {flops:,}')\n",
    "print(f'Number of parameters: {params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
